[["index.html", "알아두면 유용한 데이터 분석 방법들 들어가는 글", " 알아두면 유용한 데이터 분석 방법들 이영록 2021-03-07 들어가는 글 제가 학교 수업시간에 배우지 않았지만, 연구나 실무에서 사용했던 유용한 데이터 분석 방법들을 소개합니다. 물론 여기에 소개된 방법들 중 일부를 다루는 수업들이 여럿 존재하겠지만, 제가 접했던 개론 수업 수준에서는 거론되지 않았던 방법들이어서, 부족하게나마 쉽고 간략하게 소개를 하려고 합니다. 직관적으로 쉽게 개념을 이해할 수 있는 방법들이고, 해당 방법들의 관점을 유용하게 사용할 수 있는 현업 문제들이 존재합니다. 대부분의 데이터 분석 입문서에 설명되어 있는 방법에서 벗어나, 좀 더 독창적인 시각으로 문제를 접근하고자 하시는 분들께 도움이 되길 바랍니다. 내용은 꾸준히 업데이트될 예정입니다. "],["quantile-regression.html", "Chapter 1 Quantile regression 1.1 Univariate의 예 1.2 Quantile 1.3 Linear quantile regression 1.4 예측구간(prediction interval)", " Chapter 1 Quantile regression library(extraDistr) library(ggplot2) library(ggtext) library(ggrepel) library(gt) library(dplyr) library(tidyr) library(rlang) library(purrr) library(quantreg) library(lpSolve) 우리가 흔히 사용하는 다중회귀분석(multiple linear regression)은 예측변수 \\(X\\)를 이용하여 연속형 반응변수 \\(Y\\)의 조건부 기대값, 즉 평균을 예측하는 데 주로 사용한다. 평균은 물론 매우 유용하고 대부분의 경우 가장 중요한 통계치이나, 경우에 따라서 평균과 더불어 반응변수의 분포를 예측하는 것이 의사결정에 큰 도움이 되는 상황들이 있다. 예를 들어, 에너지 산업의 경우, 재생 에너지 생산량의 예측값보다 실제 생산량이 적었을 때 발생하는 비용(e.g. 정전 등)이 실제 생산량이 많았을 때 발생하는 비용보다 크다고 가정하면, 보수적인 관점에서 평균 반응치보다는 좀 더 낮은 변위치(quantile)을 예측하고, 그에 기반하여 예비 전력을 준비하는 것이 적합할 수 있다. 물론, 다중회귀분석 결과를 이용하여 예측값의 범위를 추정할 수 있다. 하지만, 다중회귀분석에 적용되는 가정, 즉 실제 조건부 관측치의 분포가 예측변수 \\(X\\)값에 상관없이 “동일한 분산”의 “정규분포”를 따른다는 가정은 현실 문제에서 그대로 적용되기 쉽지 않다. 이러한 가정에 위배되는 데이터에 다중회귀분석이 추정하는 예측값의 범위를 사용할 경우, 그로 인해 의사결정 과정에서 최적과는 거리가 먼 결정을 내리게 될 수 있다. Linear quantile regression은 반응변수의 분포에 대한 특정한 명시적 가정 없이, 연속형 반응변수 분포의 각 quantile과 예측변수 \\(X\\)간의 선형관계를 추정하는 방법이다. 1.1 Univariate의 예 반응변수 \\(Y\\)가 아래와 같은 Gumbel distribution을 따른다고 할 때, \\[ F(y) = \\exp \\left( -\\exp \\left( - \\frac{y - \\mu}{\\sigma} \\right) \\right) \\] \\(\\mu = 0\\), \\(\\sigma = 1\\) 에서의 분포는 아래 그래프와 같다. 위 standard Gumbel 분포에서 100,000개의 샘플을 추출한 뒤, 일반적인 최소자승 회귀분석과 quantile regression을 각각 적용하여 95% 예측구간(i.e. [2.5%, 97.5%])을 추정해보자. 이 예에서 예측변수는 존재하지 않으므로, 회귀모형은 y-절편값만을 구한다. 또한, \\(y\\)값의 실제 분포를 알고 있으므로, 실제 분포에서의 95% 구간을 구해보자. df_gumbel_random &lt;- tibble( y = rgumbel(1e5) ) true_interval &lt;- qgumbel(c(0.025, 0.975)) %&gt;% set_names(c(&quot;lwr&quot;, &quot;upr&quot;)) lm_interval &lt;- lm(y ~ 1, df_gumbel_random) %&gt;% predict(newdata = tibble(.rows = 1), interval = &quot;prediction&quot;, level = 0.95) %&gt;% drop() %&gt;% `[`(c(&quot;lwr&quot;, &quot;upr&quot;)) rq_interval &lt;- rq(y ~ 1, df_gumbel_random, tau = c(0.025, 0.975)) %&gt;% predict(newdata = tibble(.rows = 1)) %&gt;% drop() %&gt;% set_names(c(&quot;lwr&quot;, &quot;upr&quot;)) ## Warning in rq.fit.br(x, y, tau = tau, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ...): Solution may be nonunique df_prediction_interval &lt;- t(cbind(true_interval, lm_interval, rq_interval)) %&gt;% as_tibble(rownames = &quot;method&quot;) df_prediction_interval ## # A tibble: 3 x 3 ## method lwr upr ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 true_interval -1.31 3.68 ## 2 lm_interval -1.94 3.10 ## 3 rq_interval -1.31 3.69 아래 그래프에서, quantile regression을 사용하였을 때 실제 범위와 가깝게 추정되며, 일반 최소자승 회귀분석을 이용하였을 때에는 추정범위가 실제 범위로부터 멀어짐을 확인할 수 있다. 1.2 Quantile 1.2.1 정의 연속형 변수 \\(Y\\)의 분포함수를 \\(F_Y(y) = P(Y \\geq y)\\)라 할 때, \\(\\tau \\in (0, 1)\\)에 대해 \\(\\tau\\)-quantile은 아래와 같이 정의된다. \\[ Q_Y(\\tau) = F_Y^{-1}(\\tau) \\] 예를 들어, \\(Y\\)가 \\(U(0, 1)\\)의 분포를 따를 경우, 0.3-quantile은 0.3이며, \\(U(0, 2)\\)의 분포를 따른 경우, 0.3-quantile은 0.6이다. 1.2.2 Univariate 추정 분포 \\(F_Y(y)\\)로부터 \\(N\\)개의 샘플 \\(y_{(1)}, y_{(2)}, \\ldots, y_{(N)}\\)을 추출하였다 할 때 (\\(y_{(1)} \\leq y_{(2)} \\leq \\cdots \\leq y_{(N)}\\)), 분포 \\(F_Y(y)\\)의 \\(\\tau\\)-quantile을 추정하는 방법은 여러가지가 있다. R의 stats::quantile() 함수는 총 9가지 추정방법을 지원한다. 이 중, 첫 번째 방법(type = 1)에 대해 알아보자. NOTE: stats::quantile() 함수의 default는 type = 7으로 linear interpolation을 이용하며, 해당 방법이 quantile 추정에 더 적합하지만, 본 장에서 알아볼 quantile regression은 type = 1이 지원하는 quantile 추정 함수와 일관성이 있다. 우선, 분포 \\(F_Y(y)\\)를 \\(N\\)개의 샘플 \\(y_{(1)}, y_{(2)}, \\ldots, y_{(N)}\\)을 이용하여 아래와 같이 추정한다. \\[ \\hat{F}_Y(y) = \\frac{1}{N} \\sum_{i = 1}^{N} \\mathbb{I}(y_{(i)} \\leq y) \\] 예를 들어, 추출된 10개의 샘플의 값이 \\(y_{(1)} = 1, y_{(2)} = 2, \\ldots, y_{(10)} = 10\\)이었다고 할 때, 분포 \\(F_Y(y)\\)은 아래와 같이 추정된다. y &lt;- 1L:10L Fy &lt;- ecdf(y) 위에서 추정된 분포는 불연속적인 step function으로 그 역함수 \\(\\hat{F}_Y^{-1}(\\tau)\\) 가 \\(\\tau \\in (0, 1\\)에 대해 정의되지 않는다. 이에 아래와 같이 quantile을 다시 정의해보자. \\[ Q_Y(\\tau) = \\inf \\{y \\, | \\, F_Y(y) \\geq \\tau \\} \\] 이 때, \\(F_Y(y)\\) 대신 \\(\\hat{F}_Y(y)\\)를 대입하여 \\(Q_Y(\\tau)\\)를 와 같이 추정한다. \\[ \\hat{Q}_Y(\\tau) = \\inf \\{y \\, | \\, \\hat{F}_Y(y) \\geq \\tau \\} \\] quantile_Fy &lt;- Vectorize( function(tau, Fy) { q_list &lt;- eval(rlang::expr(x), environment(Fy)) p_list &lt;- eval(rlang::expr(y), environment(Fy)) q_tau &lt;- q_list[min(which(p_list &gt;= tau))] return(q_tau) }, &quot;tau&quot; ) 위에서 \\(\\hat{Q}_Y(\\tau)\\)는 \\(\\tau \\in [0, 1]\\)에서 정의되는 함수임을 볼 수 있다. 1.2.3 최적화 문제 Check function \\(\\rho_\\tau(u)\\)를 아래와 같이 정의하자. \\[\\begin{equation*} \\rho_\\tau(u) = u \\times (\\tau - \\mathbb{I}(u &lt; 0)) = \\begin{cases} \\tau |u|, &amp; u \\geq 0\\\\ (1 - \\tau) |u|, &amp; u &lt; 0 \\end{cases} \\end{equation*}\\] 이 때, 식 \\[ \\mathbf{E}[\\rho_\\tau(Y - \\xi)] = \\int_{-\\infty}^{\\infty} \\rho_\\tau(y - \\xi) f(y) \\, dy \\] 를 최소화하는 \\(\\xi\\)값은 \\(Y\\)의 \\(\\tau\\)-quantile 이다. 따라서, \\(Y\\)의 \\(N\\)개의 랜덤 샘플 \\(y_{(1)}, y_{(2)}, \\ldots, y_{(N)}\\)이 주어졌을 때, \\(\\tau\\)-quantile은 아래 목적식을 최소화하는 \\(\\xi\\)값으로 추정할 수 있다. \\[ \\frac{1}{N} \\sum_{i = 1}^{N} \\rho_\\tau(y_{(i)} - \\xi) \\] 여기에서, \\(\\frac{1}{N}\\)는 최적해를 구하는 데 영향을 미치지 않으므로, \\(\\tau\\)-quantile의 추정은 아래와 같이 표현할 수 있다. \\[ \\hat{Q}_Y(\\tau) = {\\arg\\min}_{\\xi \\in \\mathbb{R}} \\sum_{i = 1}^{N} \\rho_\\tau(y_{(i)} - \\xi) \\] 위 최적화 문제는 선형 최적화 문제로 변환할 수 있다. \\[\\begin{eqnarray*} \\min &amp;=&amp; \\sum_{i = 1}^{N} \\tau u_i^{+} + \\sum_{i = 1}^{N} (1 - \\tau) u_i^{-}\\\\ \\text{s.t.} &amp; &amp;u_i^{+} - u_i^{-} = y_{(i)} - \\xi, \\; i = 1, \\ldots, N\\\\ &amp; &amp; u_i^{+}, u_i^{-} \\geq 0, \\; i = 1, \\ldots, N \\end{eqnarray*}\\] y &lt;- 1L:10L quantile_lp &lt;- Vectorize( function(tau, y, penalty_xi = 1e-10) { n &lt;- length(y) penalty_xi &lt;- diff(range(y)) * penalty_xi obj_vec &lt;- c(penalty_xi, - penalty_xi, rep(tau, length = n), rep(1 - tau, length = n)) const_mat &lt;- cbind(rep(1L, n), rep(-1L, n), diag(n), - diag(n)) const_dir &lt;- c(rep(&quot;=&quot;)) const_rhs &lt;- y res &lt;- lp(&quot;min&quot;, obj_vec, const_mat, const_dir, const_rhs) opt_sol &lt;- res$solution[1] - res$solution[2] return(opt_sol) }, &quot;tau&quot; ) q_solution &lt;- tibble( tau = seq(0, 1, by = 0.001), q = quantile_lp(tau, y) ) NOTE: lpSolve::lp()는 모든 실수형 변수를 0보다 크거나 같다고 가정한다. 따라서, \\(\\xi \\in \\mathbb{R}\\)의 해를 구하기 위해서 \\(\\xi = \\xi^{+} - \\xi^{-}\\), \\(\\xi^{+}, \\xi^{-} \\geq 0\\) 의 방식으로 decision variable을 변환하였다. NOTE: 위 예제 데이터에 최적화 모형을 그대로 적용하면, 일부 \\(\\tau\\)값에서는 하나 이상의 최적해 값이 도출된다. 예를 들어, \\(\\tau = 0.6\\)일 때, \\(\\xi = 6\\)과 \\(\\xi = 7\\)은 동일한 목적식 값을 얻는다. 이 때, \\(\\tau\\)-quantile의 정의(\\(\\inf \\{y \\, | \\, F_Y(y) \\geq \\tau \\}\\))에 의하면 작은 값 6을 \\(\\xi\\)의 최적해로 얻어야 하나, 위 최적화 문제의 식에서는 6과 7 모두 동일하게 최적해로 판단하여 그 둘 중 하나를 lpSolve::lp() 함수의 결과값으로 리턴한다. 따라서, \\(\\tau\\)값이 step function의 knot에 해당하는 경우에는 lpSolve::lp()의 결과값이 틀린 \\(\\tau\\)-qunatile값을 리턴할 수 있다. 이를 해소하기 위해, 위에서 구현한 quantile_lp()함수에서는 목적식에 추가로 \\(\\xi\\)값에 대한 임의의 작은 패널티를 부과하였다. 이는 간단하지만 안전한 방법이 아니며, 보다 안정적인 알고리즘의 구현이 필요하다. 1.3 Linear quantile regression 1.3.1 최적화 문제 앞 장에서 살펴본 최적화 문제 \\[ \\hat{Q}_Y(\\tau) = {\\arg\\min}_{\\xi \\in \\mathbb{R}} \\sum_{i = 1}^{N} \\rho_\\tau(y_{(i)} - \\xi) \\] 는 예측변수가 존재하지 않는 경우를 가정하였다. 설명변수 \\(X \\in \\mathbb{R}^{d}\\)가 존재할 때, 주어진 예측변수에 값에 따라 \\(Y\\)의 분포가 달라진다면 (i.e. \\(F_{Y|X} \\neq F_Y\\)), \\(Y\\)의 조건부 \\(\\tau\\)-quantile \\(Q_{Y|X}(\\tau)\\)을 추정하는 접근이 필요하다. 여기에서, 조건부 \\(\\tau\\)-quantile이 아래와 같이 예측변수 \\(X = (x_1, \\, \\ldots, \\, x_d)\\)의 affine 함수라 하자. \\[ Q_{Y|X}(\\tau) = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_d x_d \\] 위 함수의 \\((d + 1)\\)개의 계수 \\(\\beta_0, \\ldots, \\beta_d\\)를 관측된 \\(N\\)개의 데이터를 통해 추정함으로써, 임의의 예측변수값에 대해 \\(Y\\)의 조건부 \\(\\tau\\)-quantile을 추정할 수 있다. \\[ \\hat{Q}_{Y|X}(\\tau) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\cdots + \\hat{\\beta}_d x_d \\] \\(i\\)번째 관측 데이터의 예측변수 관측치를 \\[ \\mathbf{x}_i = (x_{i1}, \\, \\ldots, \\, x_{id}) \\] 라 하면, 계수 \\(\\beta_0, \\ldots, \\beta_d\\)의 추정치는 아래 최적화 문제의 해로 얻어진다. \\[ \\left(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_d \\right) = {\\arg\\min}_{(\\beta_0, \\beta_1, \\ldots, \\beta_d)} \\sum_{i = 1}^{N} \\rho_\\tau(y_{(i)} - \\hat{\\beta}_0 + \\beta_1 x_{11} + \\cdots + \\beta_d x_{1d}) \\] 위 식을 아래와 같이 보다 단순화해보자. 여기에서, 회귀계수가 \\(\\tau\\)값에 따라 다르다는 것을 명시적으로 표현하기 위해 superscript \\((\\tau)\\)를 사용한다. \\[ \\boldsymbol\\beta = (\\beta_0, \\, \\beta_1, \\, \\ldots, \\beta_d) \\] \\[ \\tilde{\\mathbf{x}}_i = (1, \\, x_{i1}, \\, \\ldots, x_{id}) \\] \\[ \\hat{\\boldsymbol\\beta}^{(\\tau)} = {\\arg\\min}_{\\boldsymbol\\beta \\in \\mathbb{R}^{d + 1}} \\sum_{i = 1}^{N} \\rho_\\tau(y_{(i)} - \\tilde{\\mathbf{x}}_i^\\top \\boldsymbol\\beta) \\] 1.3.2 Intercept only 앞 절에서 살펴본 univariate 추정 결과를 R 패키지 {quantreg}내의 quantile regression 함수 rq()를 이용하여 아래와 같이 재현해볼 수 있다. 예측변수가 없는 univariate 데이터이기 때문에 (i.e. \\(d = 0\\)), 모델에서는 intercept \\(\\beta_0\\)만을 추정한다. (i.e. formula = y ~ 1). df &lt;- tibble(y = 1L:10L) rq_fit &lt;- rq(y ~ 1, data = df, tau = seq(0, 1, by = 0.001)) ## Warning in rq.fit.br(x, y, tau = tau, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ...): Solution may be nonunique q_solution1 &lt;- tibble( tau = seq(0, 1, by = 0.001), q = predict(rq_fit, tibble(.rows = 1L)) %&gt;% drop() ) 1.3.3 예측 변수가 존재하는 경우 하나의 예측변수 \\(X\\)에 대해 두 가지의 값이 관측되었고 (0, 2), 각 경우에 대해 반응변수 \\(Y\\)가 아래와 같이 10번씩 관측되었다고 하자. df &lt;- tibble( x = rep(c(0L, 2L), each = 10L), y = rep(1L:10L, times = 2L) + x ) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #nqeryosyel .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #nqeryosyel .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #nqeryosyel .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #nqeryosyel .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #nqeryosyel .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #nqeryosyel .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #nqeryosyel .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #nqeryosyel .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #nqeryosyel .gt_column_spanner_outer:first-child { padding-left: 0; } #nqeryosyel .gt_column_spanner_outer:last-child { padding-right: 0; } #nqeryosyel .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #nqeryosyel .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #nqeryosyel .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #nqeryosyel .gt_from_md > :first-child { margin-top: 0; } #nqeryosyel .gt_from_md > :last-child { margin-bottom: 0; } #nqeryosyel .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #nqeryosyel .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #nqeryosyel .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #nqeryosyel .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #nqeryosyel .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #nqeryosyel .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #nqeryosyel .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #nqeryosyel .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #nqeryosyel .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #nqeryosyel .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #nqeryosyel .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #nqeryosyel .gt_sourcenote { font-size: 90%; padding: 4px; } #nqeryosyel .gt_left { text-align: left; } #nqeryosyel .gt_center { text-align: center; } #nqeryosyel .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #nqeryosyel .gt_font_normal { font-weight: normal; } #nqeryosyel .gt_font_bold { font-weight: bold; } #nqeryosyel .gt_font_italic { font-style: italic; } #nqeryosyel .gt_super { font-size: 65%; } #nqeryosyel .gt_footnote_marks { font-style: italic; font-size: 65%; } Observed values of response variables Y in training data by value of X X = 0 X = 2 1 3 2 4 3 5 4 6 5 7 6 8 7 9 8 10 9 11 10 12 위 데이터에서, \\(X = 0\\)인 경우 반응변수의 관측치는 앞 절의 예와 동일하며, \\(X = 2\\)인 경우 반응변수의 관측치는 \\(X = 0\\)인 경우보다 각각 2씩 일정하게 높게 관측되었다. 따라서, 이 데이터를 사용하여 조건부 quantile을 추정할 때, \\(X = 0\\)인 경우에는 앞 절과 동일하게 추정되며, \\(X = 2\\)에 대하여는 \\(X = 0\\)일 경우보다 2만큼 높은 값이 각각의 \\(\\tau\\)에 대해 추정될 것을 예상할 수 있다. 즉, \\[ \\hat{\\boldsymbol\\beta}^{(\\tau)} = \\left(\\hat{\\beta}^{(\\tau)}_0, \\hat{\\beta}^{(\\tau)}_1\\right) \\] 에서, \\(\\hat{\\beta}^{(\\tau)}_1\\)의 값, 즉 예측변수 \\(X\\)에 대한 회귀계수는 \\(\\tau\\)값에 상관없이 1일 것임을 예상할 수 있다. \\(\\tau\\)값을 0부터 1까지 0.001씩 증가시키며 회귀모형을 추정해보자. tau_list &lt;- seq(0, 1, by = 0.001) rq_fit &lt;- rq(y ~ x, data = df, tau = tau_list) ## Warning in rq.fit.br(x, y, tau = tau, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ...): Solution may be nonunique 이 때, 회귀분석 결과 객체의 원소 rq_fit$coefficients는 각 열이 \\(\\tau\\)값, 각 행이 회귀계수를 나타내는 행렬이다. 여기에서 예측변수에 해당하는 회귀계수가 \\(\\tau\\)값에 상관없이 일정하게 1로 추정됨을 확인해보자. 위 그래프에서, 추정된 intercept (\\(\\hat{\\beta}_0^{\\tau}\\)) step function의 knots에 해당하는 \\(\\tau\\)값(0.1, 0.2, …, 0.9)을 제외하면, 예측변수에 해당하는 회귀계수가 \\(\\tau\\)값에 상관없이 일정하게 1로 추정되었다. TO DO: \\(\\tau\\)가 step function의 knots에 해당할 때의 quantile regression 추정에 대해 추후 내용 보충 필요. 위에서 추정된 모형을 토대로, 새로운 데이터에 대해 \\(\\tau\\)-quantile을 예측하기 위해 predict() 함수를 사용한다. 예측의 경우, 학습 데이터에서 관측되지 않았던 숫자형 예측변수 값에 대한 반응변수의 \\(\\tau\\)-quantile 또한 예측할 수 있다. 아래에서, 학습 데이터에서 관측되었던 \\(X = 0\\), \\(X = 2\\)에 더해, 학습데이터에서 관측되지 않았던 \\(X = 1\\)인 경우에 대한 \\(\\tau\\)-quantile을 예측해보자. df_predicted &lt;- tibble( x = rep(c(0L, 1L, 2L), times = length(tau_list)), tau = rep(tau_list, each = 3L), q = c(predict(rq_fit, tibble(x = c(0L, 1L, 2L))) %&gt;% as.vector()) ) 위 그래프에서, \\(X = 2\\)인 경우의 \\(\\tau\\)-quantile 예측값은 \\(X = 0\\)인 경우보다 2씩 일정하게 높음을 확인할 수 있다. 또한, \\(X = 1\\)인 경우의 \\(\\tau\\)-quantile 예측값은 \\(X = 0\\)인 경우보다 1씩 일정하게 높음을 확인할 수 있다. 1.4 예측구간(prediction interval) \\(N\\)개의 관측 데이터가 아래 모델을 통해 얻어졌다고 하자. \\[\\begin{eqnarray*} y_i &amp;=&amp; \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\\\ \\varepsilon_i &amp;\\sim&amp; F \\end{eqnarray*}\\] \\((\\beta_0, \\beta_1) = (1, 2)\\)라 하고, \\(x_i \\sim U(0, 1)\\)라 할 때, 아래의 몇 가지 다른 확률분포 \\(F\\)를 이용하여 각각 300개의 관측치가 존재하는 데이터를 생성하여 보자. \\[\\begin{eqnarray*} F_1(\\varepsilon) &amp;=&amp; N(0, 1^2)\\\\ F_2(\\varepsilon) &amp;=&amp; U(-1, 1)\\\\ F_3(\\varepsilon) &amp;=&amp; Gumbel(-\\gamma, 1)\\\\ F_4(\\varepsilon; X) &amp;=&amp; N(0, X^2)\\\\ F_5(\\varepsilon; X) &amp;=&amp; U(-X, X)\\\\ F_6(\\varepsilon; X) &amp;=&amp; Gumbel(-\\gamma(1 + X), 1 + X) \\end{eqnarray*}\\] 분포 \\(F_1, F_4\\)는 정규분포를 나타낸다. 분포 \\(F_2, F_5\\)는 uniform 분포를 나타낸다. 분포 \\(F_3, F_6\\)는 Gumbel 분포를 나타낸다. \\(\\gamma\\)는 Euler–Mascheroni 상수이다. 분포 \\(F_1, F_2, F_3\\)는 \\(X\\)값에 상관없이 동일한 분산을 지닌다. 분포 \\(F_4, F_5, F_6\\)는 \\(X\\)값이 증가함에 따라 분산이 증가한다. 분포 \\(F_1, \\ldots, F_6\\)는 모든 \\(X\\)값에 대해 조건부 평균이 0이다. 분포 \\(F_1, F_2, F_4, F_5\\)는 symmetric 분포이며, 분포 \\(F_3, F_6\\)은 right-skewed 분포이다. 위에서 분포 \\(F_1\\)을 제외한 다른 모든 분포는 최소자승 회귀분석의 가정(등분산 정규분포)을 위배함을 알 수 있다. 따라서, 최소자승 회귀분석으로부터 얻은 예측구간은 실제 예측구간을 추정하기에 적합하지 않을 것을 예상할 수 있다. beta0 &lt;- 1 beta1 &lt;- 2 euler &lt;- - digamma(1) df &lt;- tibble(x = runif(3e2, 0, 1)) %&gt;% mutate( F1 = rnorm(n(), 0, 1), F2 = runif(n(), - 1, 1), F3 = rgumbel(n(), - euler, 1), F4 = rnorm(n(), 0, x), F5 = runif(n(), - x, x), F6 = rgumbel(n(), - euler * (1 + x), 1 + x) ) %&gt;% pivot_longer(F1:F6, names_to = &quot;distribution&quot;, values_to = &quot;epsilon&quot;) %&gt;% mutate(y = beta0 + beta1 * x + epsilon) 6개 데이터셋 각각에 대해 일반 회귀분석과 quantile regression 분석을 수행해보자. 이 때, 분석의 목적은 95% 예측구간을 얻는 것이다. 일반 회귀분석의 경우, 평균을 추정하는 회귀모형이 분산 또한 추정하므로, 잘 알려진 공식에 따라 임의의 \\(x\\)값에 대한 \\(y\\)값의 예측구간을 추정할 수 있다. Quantile regression의 경우, 0.025-quantile과 0.975-quantile을 예측하는 모형을 추정한 뒤, 임의의 \\(x\\)값에 해당하는 0.025-quantile 예측값과 0.975-quantile의 예측값을 각각 95% 예측구간의 lower bound, upper bound로 간주한다. nested_df &lt;- df %&gt;% nest(data = - distribution) lm_fit &lt;- map(nested_df$data, ~ lm(y ~ x, data = .x)) rq_fit &lt;- map(nested_df$data, ~ rq(y ~ x, tau = c(0.025, 0.975), data = .x)) 이후, 각각의 데이터셋에 대해 실제 95% 구간(true_interval), 일반 최소자승 회귀모형으로부터의 95% 예측구간(lm_interval), 그리고 quantile regression을 이용하여 추정한 구간(rq_interval)을 구해보자. pred_df &lt;- tibble(x = c(0, 1, by = 0.5)) true_interval &lt;- pred_df %&gt;% mutate( F1_lwr = qnorm(0.025, 0, 1), F1_upr = qnorm(0.975, 0, 1), F2_lwr = qunif(0.025, - 1, 1), F2_upr = qunif(0.975, - 1, 1), F3_lwr = qgumbel(0.025, - euler, 1), F3_upr = qgumbel(0.975, - euler, 1), F4_lwr = qnorm(0.025, 0, x), F4_upr = qnorm(0.975, 0, x), F5_lwr = qunif(0.025, - x, x), F5_upr = qunif(0.975, - x, x), F6_lwr = qgumbel(0.025, - euler * (1 + x), 1 + x), F6_upr = qgumbel(0.975, - euler * (1 + x), 1 + x) ) %&gt;% pivot_longer(F1_lwr:F6_upr, names_to = c(&quot;distribution&quot;, &quot;quantile&quot;), names_sep = &quot;_&quot;, values_to = &quot;epsilon&quot;) %&gt;% mutate(y = beta0 + beta1 * x + epsilon) %&gt;% pivot_wider(id_cols = c(x, distribution), names_from = quantile, values_from = y) lm_interval &lt;- map2_dfr( lm_fit, paste0(&quot;F&quot;, seq_along(lm_fit)), function(object, distribution, newdata) { newdata %&gt;% mutate(distribution = distribution) %&gt;% bind_cols(as_tibble( predict(object, newdata, interval = &quot;prediction&quot;, level = 0.95))) }, newdata = pred_df ) rq_interval &lt;- map2_dfr( rq_fit, paste0(&quot;F&quot;, seq_along(lm_fit)), function(object, distribution, newdata) { newdata %&gt;% mutate(distribution = distribution) %&gt;% bind_cols(set_names(as_tibble(predict(object, newdata)), c(&quot;lwr&quot;, &quot;upr&quot;))) }, newdata = pred_df ) 위 그래프에서 보이는 바와 같이, 일반 회귀분석의 경우 회귀모형 가정을 따르는 첫 번째 데이터셋을 제외하면 실제 95% 구간과 상당한 차이를 보이는 예측구간을 추정한 반면, quantile regression은 대체로 실제 95% 구간에 근접한 예측구간을 추정하였다. "],["discrete-choice-model.html", "Chapter 2 Discrete choice model 2.1 Data 2.2 Utility 2.3 Multinomial logit (MNL) model 2.4 Nested logit model", " Chapter 2 Discrete choice model library(extraDistr) library(ggplot2) library(ggtext) library(ggrepel) library(gt) library(dplyr) library(tidyr) library(stringr) library(rlang) library(purrr) library(mlogit) 우리의 삶에는 수많은 선택의 순간들이 있다. 크게는 집을 구매할 때 어떤 집을 선택할지부터, 소소하게는 짬뽕과 짜장면 중 어떤 음식을 먹을지 선택해야한다. 물론, 가진 자원이 많다면 여러 채의 집을 구매할 수도 있고, 짬뽕과 짜장면 둘 다 주문에서 먹을 수도 있다. 하지만, 제한된 자원에서는 선택지 중 한 가지를 선택하면 나머지는 포기해야 하는 상황에 부딪힌다. 이 때, 사람들이 어떤 선택을 하는지를 모델링하는 방법이 있다. 본 장에서는 discrete choice model이라 불리는 방법에 대해 알아보자. 2.1 Data 아래와 같이 데이터를 정의해보자. 2.1.1 Choice \\(j = 1, \\ldots, J\\): 선택할 수 있는 \\(J\\)개의 서로 다른 대안(alternative). 예를 들어, 짬뽕(\\(j = 1\\))과 짜장면(\\(j = 2\\)). \\(i = 1, \\ldots, N\\): 선택하여야 하는 상황(choice situation). 예를 들어, 100명의 손님이 있다면, 각각의 손님(\\(i = 1, \\ldots, 100\\))은 짬뽕과 짜장면 중 한 가지 음식을 선택하여야 한다. \\(y_{ij}\\): \\(i\\)번째 선택 상황에서 \\(j\\)번째 대안이 선택되었다면 1, 아니라면 0. 이 때, \\(y_{ij}\\)는 모든 \\(i\\)에 대해 다음과 같은 제약을 만족한다. \\[ \\sum_{j = 1}^{J} y_{ij} = 1\\\\ y_{ij} \\in \\{0, 1\\} \\] 2.1.2 Covariate 선택 변수 \\(y_{ij}\\)의 값은 선택 상황에 대한 설명 변수들과 상관관계가 있을 수 있다. 예를 들어, 손님의 나이가 어릴수록 짜장면을 선택할 확률이 높다거나, 날씨가 비가 오면 짬뽕을 선택할 확률이 높다거나, 음식 가격이 비쌀수록 해당 음식을 선택할 확률이 줄어들 수 있다. 이러한 변수의 벡터를 아래 \\(\\mathbf{x}_{ij}\\)와 \\(\\mathbf{z}_{i}\\) 두 가지로 나누어 정의하자. \\(\\mathbf{x}_{ij}\\): \\(i\\)번째 선택 상황에서 \\(j\\)번째 대안에 대한 설명변수 관측치 (음식 가격 등) \\(\\mathbf{z}_{i}\\): \\(i\\)번째 선택 상황에 대한 설명변수 관측치 (손님 나이, 날씨 등) 위에서 \\(\\mathbf{x}_{ij}\\)는 이후에 다시 두 가지 벡터로 분리된다. 2.1.3 Availability \\(i\\)번째 선택 상황에서 \\(j\\)번째 대안이 선택 가능하지 않는 경우들이 있다. 예를 들어, 재료수급 문제로 짬뽕이 품절이었거나, 손님이 가진 현금이 짬뽕을 주문하기 부족한 경우, 짬뽕은 선택 가능한 대안에서 제외되게 된다. 이를 표현하기 위해 변수 \\(a_{ij}\\)를 아래와 같이 정의하자. \\(a_{ij}\\): \\(i\\)번째 선택 상황에서 \\(j\\)번째 대안이 선택 가능했다면 1, 아니라면 0. 이 때, \\(a_{ij}\\) 와 \\(y_{ij}\\)간에는 아래와 같은 관계가 성립한다. \\[ a_{ij} \\geq y_{ij} \\] 따라서, \\[ \\sum_{j = 1}^{J} a_{ij} \\geq \\sum_{j = 1}^{J} y_{ij} = 1 \\] 즉, 적어도 하나의 대안은 선택 가능하여야 한다. \\(i\\)번째 선택 상황에서 선택 가능한 대안의 집합을 \\(A_i\\)라 하자. \\[ A_i = \\left\\{j : a_{ij} = 1 \\right\\} \\] 이 때, \\(y_{ij}\\)값은 아래와 같은 조건을 만족한다. \\[ \\sum_{j \\in A_i} y_{ij} = 1\\\\ y_{ij} \\in \\{0, 1\\}, \\; \\forall j \\in A_i\\\\ y_{ij} = 0, \\; \\forall j \\notin A_i\\\\ \\] 2.1.4 “No choice” alternative 앞서 availability 측면에서 고려했던, 짬뽕이 선택 가능하지 않고 짜장면만 선택 가능한 경우, \\(\\sum_{j = 1}^{J} y_{ij} = 1\\)에 따라 무조건 짜장면을 선택하게 된다. 하지만, 짬뽕이 선택 가능하지 않았을 때, 짜장면을 먹는 대신 다른 음식점을 찾아가는 경우를 예상해볼 수 있다. 이 때, 손님은 선택 가능한 대안 중 어떠한 선택도 하지 않은 것이 되어 \\(\\sum_{j = 1}^{J} y_{ij} = 1\\)을 만족하지 못한다. 이 때, 해당 선택 상황을 분석에서 제외하는 대신, 모든 선택 상황에서 \\(J\\)개의 대안 중 어떠한 대안도 선택하지 않는 \\(J + 1\\)째의 대안을 고려할 수 있다. 이에 대한 구체적은 예는 이후 다시 다루도록 하자. 2.1.5 데이터 예 손님ID 손님나이 음식 가격 선택여부 1 40 짬뽕 8000 1 1 40 짜장면 7000 0 2 10 짬뽕 5000 0 2 10 짜장면 5000 1 3 70 짜장면 6500 1 2.2 Utility \\(i\\)번째 선택 상황에서 \\(j\\)번째 대안에 대한 효용성의 값을 \\(u_{ij} \\in \\mathbb{R}\\)이라 하자. 이 때, 합리적인 선택은 효용성이 가장 큰 대안을 선택하는 것이다. 본 장에서 다룰 기본적인 discrete choice model들은 이러한 합리적인 선택을 가정한다. \\[ y_{ij} = \\begin{cases} 1 &amp; j = \\arg\\max_{j \\in A_i} u_{ij}\\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] 이 때, \\(u_{ij}\\)는 관측되지 않는 값이다. 이를 두 개의 부분으로 아래와 같이 나눈다. \\[ u_{ij} = v_{ij} + \\varepsilon_{ij} \\] 이 때, \\(v_{ij}\\)는 관측된 설명변수 \\(\\mathbf{x}_{ij}\\)와 \\(\\mathbf{z}_i\\)에 대한 함수이며, \\(\\varepsilon_{ij}\\)는 설명변수로는 설명되지 않는 부분이다. \\[ v_{ij} = f(\\mathbf{x}_{ij}, \\mathbf{z}_i) \\] 이 때, 함수 \\(f()\\)의 형태나 \\(\\varepsilon_{ij}\\)에 대한 가정에 따라 다양한 형태의 discrete choice model이 존재한다. 이후 본 장에서는 가장 간단한 두 가지 모형들(multinomial logit model, nested logit model)을 살펴보기로 하자. 2.2.1 Observable utility (Representative utility) \\(v_{ij}\\)는 observable utility 혹은 representative utility라 한다. \\(v_{ij}\\)를 설명변수에 대한 affine 함수로 아래와 같이 정의하자. \\[ v_{ij} = \\alpha_j + \\boldsymbol{\\beta}^{\\top} \\mathbf{x}_{ij} + \\boldsymbol{\\gamma}_j^{\\top} \\mathbf{z}_i + \\boldsymbol{\\delta}_j \\mathbf{x}_{ij} \\] 이 때, \\(\\boldsymbol{\\beta}\\)는 모든 대안 \\(j\\)에 동일하게 적용되는 계수벡터이며, \\(\\boldsymbol{\\gamma}_j\\)와 \\(\\boldsymbol{\\delta}_j\\)는 대안 \\(j\\)에 따라 다른 값을 지니는 계수벡터이다. 변수 벡터 \\(\\mathbf{x}_{ij}\\)내의 각 변수는 \\(\\boldsymbol{\\beta}\\)와 \\(\\boldsymbol{\\delta}_j\\) 중 한 가지의 회귀계수에만 해당한다고 가정하고, 설명변수 벡터 \\(\\mathbf{w}_{ij}\\)를 추가로 정의하자. \\[ v_{ij} = \\alpha_j + \\boldsymbol{\\beta}^{\\top} \\mathbf{x}_{ij} + \\boldsymbol{\\gamma}_j^{\\top} \\mathbf{z}_i + \\boldsymbol{\\delta}_j \\mathbf{w}_{ij} \\] 여기에서, 각각의 변수에 대한 예는 아래와 같이 생각해볼 수 있다. \\(\\mathbf{x}_{ij}\\): (예: 음식가격) 음식 가격은 각 메뉴별로 다르나, 손님이 느끼는 효용성에 가격이 미치는 영향은 단위 가격당 동일한 경우, 음식 가격에 적용되는 계수 \\(\\beta\\)의 값은 대안 \\(j\\)와 상관없이 동일할 수 있다. \\(\\mathbf{z}_{i}\\): (예: 나이) 나이가 어릴수록 짜장면을 선호하고 짬뽕을 덜 선호하는 경향이 커지는 경우, 나이에 적용되는 계수 \\(\\gamma\\)의 값은 짬뽕(\\(j = 1\\))에 대한 계수값보다 짜장면(\\(j = 2\\))에 대한 계수값이 크다고 볼 수 있다 (\\(\\gamma_1 &lt; \\gamma_2\\)). \\(\\mathbf{w}_{ij}\\): (예: 음식가격) 만약 음식에 느끼는 효용성에 가격이 미치는 영향이 메뉴별로 다를 경우, 음식 가격은 \\(\\mathbf{x}_{ij}\\)가 아닌 \\(\\mathbf{w}_{ij}\\)에 해당하는 변수라 할 수 있다. 짜장면 가격이 500원 증가할 때 느끼는 효용성의 감소가 짬뽕 가격이 500원 증가할 때 느끼는 효용성의 감소보다 크다면, 즉 효용성 가격 민감도가 짜장면의 경우가 더 크다면, \\(\\delta_2 &lt; \\delta_1 &lt; 0\\)라 볼 수 있다 (\\(|\\delta_2| &gt; |\\delta_1|\\)). 위의 세 가지 변수 종류 중 본 장에서는 두 가지 변수 \\(\\mathbf{x}_{ij}\\)와 \\(\\mathbf{z}_{i}\\)를 중심으로 살펴보자. 아래와 같이 보다 단순한 형태의 \\(v_{ij}\\)를 고려하도록 하자. \\[ v_{ij} = \\alpha_j + \\boldsymbol{\\beta}^{\\top} \\mathbf{x}_{ij} + \\boldsymbol{\\gamma}_j^{\\top} \\mathbf{z}_i \\] 여기에서, \\(\\mathbf{x}_{ij} \\in \\mathbb{R}^r\\), \\(\\mathbf{z}_{i} \\in \\mathbb{R}^s\\)라 할 때, 모든 \\((i, j)\\)에 대한 \\(v_{ij}\\)는 아래 세 가지 파라미터를 이용한 함수이다. \\[ \\boldsymbol{\\alpha} = (\\alpha_1, \\ldots, \\alpha_J) \\in \\mathbb{R}^J\\\\ \\boldsymbol{\\beta} = (\\beta_1, \\ldots, \\beta_r) \\in \\mathbb{R}^r\\\\ \\boldsymbol{\\gamma} = (\\gamma_{11}, \\ldots, \\gamma_{1s}, \\gamma_{21}, \\ldots, \\gamma_{Js}) \\in \\mathbb{R}^{J \\times s} \\] 이 때, 주어진 데이터로부터 회귀계수들의 추정치 \\(\\hat{\\boldsymbol{\\alpha}}, \\hat{\\boldsymbol{\\beta}}, \\hat{\\boldsymbol{\\gamma}}\\)을 구하는 것이 discrete choice model 추정이다. 2.2.2 Unobservable utility \\(\\varepsilon_{ij}\\)는 효용성 중 covariate으로 설명되지 않는 부분이다. 같은 나이의 두 손님이 동일한 가게에서 동일한 시각에 각자 식사를 하더라도, 한 명은 짬뽕을 선택하고 다른 한 명은 짜장면을 선택할 수 있다. 이는 설명변수(나이, 가격 등)로 설명되지 않는 각 손님의 개인적인 취향에 기인한 것으로 생각할 수 있다. 임의의 \\(i\\)번째 선택 상황에서, \\(j \\in A_i\\)에 대해 \\(v_{ij}\\)의 참값은 알고 있고 \\(y_{ij}\\)값은 관측되지 않았다 할 때, \\(y_{ij}\\)에 대한 기대값은 아래와 같이 표현할 수 있다. \\[\\begin{eqnarray*} E[y_{ij}] &amp;=&amp; \\prod_{k \\in A_i \\backslash j} P(u_{ij} &gt; u_{ik})\\\\ &amp;=&amp; \\prod_{k \\in A_i \\backslash j} P(v_{ij} + \\varepsilon_{ij} &gt; v_{ik} + \\varepsilon_{ik})\\\\ &amp;=&amp; \\prod_{k \\in A_i \\backslash j} P(\\varepsilon_{ij} - \\varepsilon_{ik} &gt; v_{ik} - v_{ij}) \\end{eqnarray*}\\] 여기서, \\(y_{ij}\\)의 기대값을 \\(p_{ij}\\)라 하면 (\\(p_{ij} = E[y_{ij}]\\)), \\(p_{ij}\\)는 아래의 식들을 만족한다. \\[ \\sum_{j \\in A_i} p_{ij} = 1\\\\ p_{ij} \\in [0, 1] \\] \\(y_{ij}\\) (\\(j = 1, \\ldots, J\\))는 확률 \\(p_{ij}\\) (\\(j = 1, \\ldots, J\\))로 정의된 multinomial distribution으로부터 얻어진 관측치라 하자. \\[ (y_{i1}, \\ldots, y_{iJ}) \\sim multinom(p_{i1}, \\ldots, p_{iJ}) \\] 위에서, \\(\\varepsilon_{ij}\\)의 분포에 대한 가정에 따라 \\(p_{ij}\\)의 값이 다르게 추정된다. 따라서 discrete choice model 추정에서는 observable utility에 대한 model specification 뿐만 아니라 unobservable utility에 대한 분포 가정 또한 중요하다. 다음 장에서 살펴볼 multinomial logit model과 nested logit model은 \\(\\varepsilon_{ij}\\)의 분포에 대해 서로 다른 가정을 지닌다. 2.2.3 Maximum likelihood estimation 각 선택상황 \\(i = 1, \\ldots, N\\)에서의 선택이 서로 독립이라 할 때, 파리미터값 \\(\\boldsymbol{\\alpha}, \\boldsymbol{\\beta}, \\boldsymbol{\\gamma}\\)에 대한 우도함수(likelihood function)는 아래와 같이 표현된다. \\[ L(\\boldsymbol{\\alpha}, \\boldsymbol{\\beta}, \\boldsymbol{\\gamma}) = \\prod_{i = 1}^{N} \\prod_{j = 1}^{J} p_{ij} ^ {y_{ij}} \\] 따라서, 로그우도함수(log-likelihood function)는 아래와 같이 표현된다. \\[ l(\\boldsymbol{\\alpha}, \\boldsymbol{\\beta}, \\boldsymbol{\\gamma}) = \\sum_{i = 1}^{N} \\sum_{j = 1}^{J} y_{ij} \\log p_{ij} \\] 이때, 파리미터 \\(\\boldsymbol{\\alpha}, \\boldsymbol{\\beta}, \\boldsymbol{\\gamma}\\)는 위 로그우도함수를 최대화하는 값으로 추정할 수 있다. \\[ \\{\\hat{\\boldsymbol{\\alpha}}, \\hat{\\boldsymbol{\\beta}}, \\hat{\\boldsymbol{\\gamma}}\\} = {\\arg\\max}_{\\boldsymbol{\\alpha}, \\boldsymbol{\\beta}, \\boldsymbol{\\gamma}} l(\\boldsymbol{\\alpha}, \\boldsymbol{\\beta}, \\boldsymbol{\\gamma}) \\] 위에서 우도함수 및 로그우도함수를 \\(y_{ij}\\)와 \\(p_{ij}\\)를 이용하여 표현한 식은 multiclass classification 문제에서 널리 볼 수 있다 (명목형 로지스틱 회귀모형 등). 결국, \\(p_{ij}\\)의 모형이 어떻게 정의되는지에 그 차이가 존재한다. 다음 절에서는 구체적으로 \\(p_{ij}\\)의 모형을 살펴보기로 한다. 2.3 Multinomial logit (MNL) model Discrete choice model을 얘기할 때 가장 기본으로 다루는 모형이다. 2.3.1 Utility \\[ u_{ij} = v_{ij} + \\varepsilon_{ij} \\] 에서, unobservable utility \\(\\varepsilon_{ij}\\)가 standard Gumbel distribution으로부터 얻어진다고 가정한다. \\[ \\varepsilon_{ij} \\overset{i.i.d.}{\\sim} Gumbel(0, 1) \\] 이 때, standard Gumbel distribution은 아래와 같다. \\[ f(\\varepsilon_{ij}) = \\exp\\left(-\\varepsilon_{ij}\\right) \\exp\\left(-\\exp\\left(-\\varepsilon_{ij}\\right)\\right)\\\\ F(\\varepsilon_{ij}) = \\exp\\left(-\\exp\\left(-\\varepsilon_{ij}\\right)\\right) \\] 2.3.2 Choice probablity 모든 대안 \\(j\\)에 대해 observable utility \\(v_{ij}\\)와 alternative availability \\(a_{ij}\\)가 주어진 상황에서, \\(j\\)번째 대안이 선택될 확률 \\(p_{ij}\\)는 아래와 같이 계산된다. \\[ p_{ij} = \\begin{cases} \\frac{\\exp(v_{ij})}{\\sum_{k \\in A_i} \\exp(v_{ik})} &amp; \\text{if } a_{ij} = 1,\\\\ 0 &amp; \\text{otherwise}. \\end{cases} \\] 2.3.2.1 Choice probablity 계산 여기에서는 위 \\(p_{ij}\\)가 계산되는 식을 유도해본다. 임의의 \\(j, k \\in A_i, \\, j \\neq k\\)에 대해서, \\[ P(v_{ij} + \\varepsilon_{ij} &gt; v_{ik} + \\varepsilon_{ik}) = P(\\varepsilon_{ik} &lt; \\varepsilon_{ij} + v_{ij} - v_{ik}) \\] 여기서 \\(\\varepsilon_{ij}\\)값이 주어졌다고 가정하면, 아래와 같이 \\(\\varepsilon_{ik}\\)의 조건부 확률분포가 Gumbel distribution으로 얻어진다. \\[ P(\\varepsilon_{ik} &lt; \\varepsilon_{ij} + v_{ij} - v_{ik} \\, | \\, \\varepsilon_{ij}) = \\exp\\left(-\\exp\\left(-\\left(\\varepsilon_{ij} + v_{ij} - v_{ik}\\right)\\right)\\right) \\] 띠라서, \\(\\varepsilon_{ij}\\)값이 주어졌을 때 \\(j\\)번째 대안이 선택될 조건부 확률은 아래와 같다. \\[ \\prod_{k \\in A_i \\backslash j} P(\\varepsilon_{ik} &lt; \\varepsilon_{ij} + v_{ij} - v_{ik} \\, | \\, \\varepsilon_{ij}) = \\prod_{k \\in A_i \\backslash j} \\exp\\left(-\\exp\\left(-\\left(\\varepsilon_{ij} + v_{ij} - v_{ik}\\right)\\right)\\right) \\] 이 조건부 확률을 \\(\\varepsilon_{ij}\\)의 분포를 적용하여 적분하면, \\(\\varepsilon_{ij}\\)값이 주어지지 않은 상태에서 \\(j\\)번째 대안이 선택될 확률을 구할 수 있다. \\[ p_{ij} = \\int_{-\\infty}^{\\infty} \\left( \\prod_{k \\in A_i \\backslash j} \\exp\\left(-\\exp\\left(-\\left(\\varepsilon_{ij} + v_{ij} - v_{ik}\\right)\\right)\\right) \\right) f(\\varepsilon_{ij}) \\, d\\varepsilon_{ij}\\\\ = \\int_{-\\infty}^{\\infty} \\left( \\prod_{k \\in A_i \\backslash j} \\exp\\left(-\\exp\\left(-\\left(\\varepsilon_{ij} + v_{ij} - v_{ik}\\right)\\right)\\right) \\right) \\exp\\left(-\\varepsilon_{ij}\\right) \\exp\\left(-\\exp\\left(-\\varepsilon_{ij}\\right)\\right) \\, d\\varepsilon_{ij} \\] 여기에서, \\(v_{ij} - v_{ij} = 0\\)임을 이용하면, \\[ p_{ij} = \\int_{-\\infty}^{\\infty} \\left( \\prod_{k \\in A_i} \\exp\\left(-\\exp\\left(-\\left(\\varepsilon_{ij} + v_{ij} - v_{ik}\\right)\\right)\\right) \\right) \\exp\\left(-\\varepsilon_{ij}\\right) \\, d\\varepsilon_{ij}\\\\ = \\int_{-\\infty}^{\\infty} \\exp \\left(-\\sum_{k \\in A_i} \\exp\\left(-\\left(\\varepsilon_{ij} + v_{ij} - v_{ik}\\right)\\right)\\right) \\exp\\left(-\\varepsilon_{ij}\\right) \\, d\\varepsilon_{ij}\\\\ = \\int_{-\\infty}^{\\infty} \\exp \\left(-\\exp(-\\varepsilon_{ij}) \\sum_{k \\in A_i} \\exp\\left(-\\left(v_{ij} - v_{ik}\\right)\\right)\\right) \\exp\\left(-\\varepsilon_{ij}\\right) \\, d\\varepsilon_{ij} \\] 이 때, \\(t = \\exp(-\\varepsilon_{ij})\\)라 하면 (\\(t \\in (0, \\infty)\\)), \\(dt = - \\exp(-\\varepsilon_{ij}) \\, d\\varepsilon_{ij}\\)임을 이용하여 아래와 같이 식을 전개할 수 있다. \\[ p_{ij} = \\int_{\\infty}^{0} \\exp \\left(-t \\sum_{k \\in A_i} \\exp\\left(-\\left(v_{ij} - v_{ik}\\right)\\right)\\right) \\, (-dt)\\\\ = \\int_{0}^{\\infty} \\exp\\left(-t \\sum_{k \\in A_i} \\exp\\left(-\\left(v_{ij} - v_{ik}\\right)\\right)\\right) \\, dt\\\\ = \\left. \\frac{\\exp\\left(-t \\sum_{k \\in A_i} \\exp\\left(-\\left(v_{ij} - v_{ik}\\right)\\right)\\right)}{-\\sum_{k \\in A_i} \\exp\\left(-\\left(v_{ij} - v_{ik}\\right)\\right)} \\right|_{0}^{\\infty}\\\\ = \\frac{1}{\\sum_{k \\in A_i} \\exp\\left(-\\left(v_{ij} - v_{ik}\\right)\\right)}\\\\ = \\frac{\\exp(v_{ij})}{\\sum_{k \\in A_i} \\exp(v_{ik})} \\] 2.3.3 Independent from irrelevant alternatives (IIA) Multinomial logit model에서는 서로 다른 두 개의 대안 \\(j\\)와 \\(k\\)간의 상대적인 선택확률(\\(p_{ij} / p_{ik}\\))은 또 다른 대안 \\(l \\notin \\{j, k\\}\\)을 선택할 확률 \\(p_{il}\\)에 영향을 받지 않는다고 가정한다. \\[ \\frac{p_{ij}}{p_{ik}} = \\frac{\\exp(v_{ij})}{\\exp(v_{ik})} \\] 이는 어떠한 두 대안이 다른 대안들보다 더 강한 대체관계를 가지지는 않는다는 것을 의미한다. 현실의 선택상황에서는 이러한 가정이 성립하지 않는 경우가 많으며, 이에 대한 방법은 다음 nested logit model절에서 더 다루기로 한다. 2.3.4 예제 Multinomial logit model의 추정과 그로부터 확률을 추정하는 과정을 아래 예제 데이터와 R 스크립트로 살펴보도록 하자. 2.3.4.1 데이터 모델 네 가지 메뉴가 있는 식당을 고려해보자. 각 선택 상황에 따라 가격과 메뉴 주문 가능 여부는 매번 달라진다고 가정하자. 대안 \\(j = 1\\): 짬뽕 \\(j = 2\\): 짜장면 \\(j = 3\\): 삼선짬뽕 \\(j = 4\\): 삼선짜장 설명변수 \\(x_{ij}\\): \\(i\\)번째 손님이 \\(j\\)번째 음식에 대해 지불해야 할 가격 \\[\\begin{eqnarray*} x_{i1} &amp;\\sim&amp; U(5000, 7000)\\\\ x_{i2} &amp;\\sim&amp; U(5000, x_{i1})\\\\ x_{i3} &amp;\\sim&amp; x_{i1} + U(2000, 4000)\\\\ x_{i4} &amp;\\sim&amp; x_{i2} + (x_{i3} - x_{i1}) + U(-500, 0) \\end{eqnarray*}\\] \\(z_{i}\\): \\(i\\)번째 손님의 나이 \\[ z_i \\sim U(5, 95) \\] Observable utility \\[\\begin{eqnarray*} v_{i1} &amp;=&amp; 0 - 0.001 \\times x_{i1} + 0 \\times z_{i}\\\\ v_{i2} &amp;=&amp; 4 - 0.001 \\times x_{i2} - 0.1 \\times z_{i}\\\\ v_{i3} &amp;=&amp; 2 - 0.001 \\times x_{i3} + 0 \\times z_{i}\\\\ v_{i4} &amp;=&amp; 6 - 0.001 \\times x_{i4} - 0.1 \\times z_{i} \\end{eqnarray*}\\] Unobservable utility \\(\\varepsilon_{ij} \\overset{i.i.d.}{\\sim} Gumbel(0, 1)\\) Availability \\[\\begin{eqnarray*} a_{i1} &amp;=&amp; 1\\\\ a_{i2} &amp;=&amp; 1\\\\ a_{i3} &amp;\\sim&amp; Bernoulli(0.8)\\\\ a_{i4} &amp;\\sim&amp; Bernoulli(0.8) \\end{eqnarray*}\\] set.seed(2) N &lt;- 2000 menu &lt;- c(&quot;짬뽕&quot; = 1, &quot;짜장면&quot; = 2, &quot;삼선짬뽕&quot; = 3, &quot;삼선짜장&quot; = 4) menu_name &lt;- names(menu) alpha_true &lt;- c(0, 4, 2, 6) beta_true &lt;- -0.001 gamma_true &lt;- c(0, -0.1, 0, -0.1) generate_price &lt;- function(N) { tibble( i = seq_len(N), price1 = runif(N, 5000, 7000), price2 = runif(N, 5000, price1), price3 = price1 + runif(N, 2000, 4000), price4 = price2 + price3 - price1 + runif(N, -500, 0) ) %&gt;% pivot_longer( cols = price1:price4, names_to = &quot;alternative&quot;, names_prefix = &quot;price&quot;, names_transform = list(alternative = as.integer), values_to = &quot;price&quot; ) } generate_age &lt;- function(N) { tibble( i = seq_len(N), age = runif(N, 5, 95) ) } generate_availability &lt;- function(N) { tibble( i = seq_len(N), avail1 = 1, avail2 = 1, avail3 = avail1 * rbern(N, 0.8), avail4 = avail2 * rbern(N, 0.8) ) %&gt;% pivot_longer( cols = avail1:avail4, names_to = &quot;alternative&quot;, names_prefix = &quot;avail&quot;, names_transform = list(alternative = as.integer), values_to = &quot;avail&quot; ) } choice_df &lt;- generate_price(N) %&gt;% inner_join(generate_age(N), by = &quot;i&quot;) %&gt;% inner_join(generate_availability(N), by = c(&quot;i&quot;, &quot;alternative&quot;)) %&gt;% filter(avail == 1) %&gt;% mutate( v = alpha_true[alternative] + beta_true * price + gamma_true[alternative] * age, e = rgumbel(n()), u = v + e ) %&gt;% mutate(menu_item = menu_name[alternative]) %&gt;% select(-avail) %&gt;% group_by(i) %&gt;% mutate(choice = if_else(u == max(u), 1L, 0L)) %&gt;% ungroup() 2.3.4.2 모델 추정 R 패키지 {mlogit}의 함수 mlogit()을 이용하여 multinomial logit model을 추정할 수 있다. fit_mnl &lt;- mlogit(choice ~ price | age, data = choice_df, alt.var = &quot;alternative&quot;, chid.var = &quot;i&quot;) estimate_df &lt;- tibble( term = names(fit_mnl$coefficients), estimate = coef(fit_mnl), std.error = sqrt(diag(solve(-fit_mnl$hessian))), true = c(alpha_true[-1], beta_true, gamma_true[-1]) ) term estimate std.error true (Intercept):2 3.8375930 0.2096440 4.000 (Intercept):3 2.3640126 0.3578062 2.000 (Intercept):4 5.8438709 0.3064476 6.000 price -0.0010338 0.0000823 -0.001 age:2 -0.0985531 0.0043177 -0.100 age:3 -0.0045850 0.0038677 0.000 age:4 -0.0930937 0.0049826 -0.100 위 추정 결과, 각 파라미터에 대해 추정된 95% 신뢰구간들은 실제 데이터 모델에서 사용된 파라미터값(true values)을 포함하고 있음을 확인할 수 있다. 2.3.4.3 확률 추정 추정된 모형에 기반하여, 다음과 같은 메뉴판이 주어졌을 때 손님의 선택을 예측해보자. alpha_hat &lt;- c(0, coef(fit_mnl)[c(&quot;(Intercept):2&quot;, &quot;(Intercept):3&quot;, &quot;(Intercept):4&quot;)]) beta_hat &lt;- coef(fit_mnl)[c(&quot;price&quot;)] gamma_hat &lt;- c(0, coef(fit_mnl)[c(&quot;age:2&quot;, &quot;age:3&quot;, &quot;age:4&quot;)]) new_choice_situation &lt;- tribble( ~menu_item, ~price, &quot;짬뽕&quot;, 7000, &quot;짜장면&quot;, 6000, &quot;삼선짬뽕&quot;, 10000, &quot;삼선짜장&quot;, 9000 ) %&gt;% mutate( alternative = menu[menu_item] ) menu_item price alternative 짬뽕 7000 1 짜장면 6000 2 삼선짬뽕 10000 3 삼선짜장 9000 4 이 때, 손님의 나이가 15세인 경우의 선택확률 예측은 아래와 같다. pred_df1 &lt;- new_choice_situation %&gt;% mutate(age = 15) %&gt;% mutate( v = alpha_hat[alternative] + beta_hat * price + gamma_hat[alternative] * age, p = exp(v - max(v)) / sum(exp(v - max(v))) ) NOTE: 확률을 계산하는 식을 p = exp(v) / sum(exp(v)) 대신 p = exp(v - max(v)) / sum(exp(v - max(v)))로 구현하였다. 두 계산방법은 이론적으로 동일한 확률값을 나타낸다. 컴퓨팅 과정에서 v값이 매우 크거나 매우 작을 경우, exp(v)값이 Inf 혹은 0으로 계산되어 확률값이 적절하게 계산되지 않을 수 있기 때문에, 그러한 가능성을 줄이기 위해서 exp(v) 대신 exp(v - max(v))을 사용하였다. 반면, 손님의 나이가 45세인 경우의 선택확률 예측은 아래와 같다. pred_df2 &lt;- new_choice_situation %&gt;% mutate(age = 45) %&gt;% mutate( v = alpha_hat[alternative] + beta_hat * price + gamma_hat[alternative] * age, p = exp(v - max(v)) / sum(exp(v - max(v))) ) 위 손님의 나이가 45세인 경우, 만약 삼선짜장이 메뉴에서 제외된다면, 선택확률은 아래와 같이 달라진다. pred_df3 &lt;- new_choice_situation %&gt;% filter(menu_item != &quot;삼선짜장&quot;) %&gt;% mutate(age = 45) %&gt;% mutate( v = alpha_hat[alternative] + beta_hat * price + gamma_hat[alternative] * age, p = exp(v - max(v)) / sum(exp(v - max(v))) ) 위에서 각 메뉴의 선택 확률은 기존 확률에 비례하여 증가한다. 따라서, 위 삼선짜장이 제외된 메뉴에서의 선택 확률의 예측은, 아래와 같이 삼선짜장이 포함된 메뉴에서 손님이 각 메뉴를 선택할 확률 예측값으로부터 산출할 수 있다. pred_df4 &lt;- pred_df2 %&gt;% filter(menu_item != &quot;삼선짜장&quot;) %&gt;% mutate(p = p / sum(p)) 2.3.4.4 주어진 대안들을 선택하지 않을 확률 위 주어진 메뉴판에서 각 메뉴의 가격이 모두 3만원씩 올랐다고 가정해보자. new_choice_situation_2 &lt;- tribble( ~menu_item, ~price, &quot;짬뽕&quot;, 37000, &quot;짜장면&quot;, 36000, &quot;삼선짬뽕&quot;, 40000, &quot;삼선짜장&quot;, 39000 ) %&gt;% mutate( alternative = menu[menu_item] ) menu_item price alternative 짬뽕 37000 1 짜장면 36000 2 삼선짬뽕 40000 3 삼선짜장 39000 4 pred_df5 &lt;- new_choice_situation_2 %&gt;% mutate(age = 45) %&gt;% mutate( v = alpha_hat[alternative] + beta_hat * price + gamma_hat[alternative] * age, p = exp(v - max(v)) / sum(exp(v - max(v))) ) 위 결과와 같이, 각 메뉴를 선택할 확률은 모든 메뉴의 가격을 3만원씩 올리기 이전과 동일하다. 이는 utility가 가격에 따라 linear하게 증가하여, 두 메뉴의 가격의 차이가 변하지 않을 때는 두 메뉴 간의 효용성의 차이 또한 변하지 않기 때문이다. \\[ \\frac{\\exp(v_{ij} + \\beta \\times 30000)}{\\exp(v_{ik} + \\beta \\times 30000)} = \\frac{\\exp(v_{ij})\\exp(\\beta \\times 30000)}{\\exp(v_{ik})\\exp(\\beta \\times 30000)} = \\frac{\\exp(v_{ij})}{\\exp(v_{ik})} = \\frac{p_{ij}}{p_{ik}} \\] 위의 확률은 네 가지 음식 중 한 가지가 무조건 선택된다는 가정 하에서 추정되는 확률이다. 하지만, 위처럼 가격이 크게 오른 새 메뉴판이 주어진다면 많은 손님들은 어떠한 음식도 선택하지 않고 음식점을 나설 것이다. 이렇게 “어떠한 음식도 선택하지 않을” 확률을 추정하기 위해서는 “어떠한 음식도 선택하지 않음”을 또 하나의 대안으로 고려한 모형 추정이 필요하다. 이를 위해서는 선택 모형을 추정하는데 필요한 학습데이터에 음식점에 방문하였으나 음식을 주문하지 않고 나간 손님에 대한 데이터가 존재해야 한다. 위 네 가지 음식 외에 추가로 “안 먹어”라는 대안(\\(j = 5\\))을 설정하고, 해당 대안에 대한 데이터 모델이 아래와 같다고 가정해보자. \\[\\begin{eqnarray*} x_{i5} &amp;=&amp; 0\\\\ v_{i5} &amp;=&amp; -10\\\\ \\varepsilon_{i5} &amp;\\overset{i.i.d.}{\\sim}&amp; Gumbel(0, 1)\\\\ a_{i5} &amp;=&amp; 1 \\end{eqnarray*}\\] 즉, 어떠한 음식도 선택하지 않았을 때 손님이 지불해야 할 가격은 0원이며 (\\(x_{i5} = 0\\)), 이 때의 효용성은 손님의 나이에 상관이 없고 (\\(v_{i5} = -10\\)), 다만 손님에 따라 어떠한 음식도 선택하지 않을 때 느끼는 효용성은 조금씩 다르며 (\\(\\varepsilon_{i5} \\overset{i.i.d.}{\\sim} Gumbel(0, 1)\\)), 어떠한 음식도 선택하지 않을 권리는 항상 손님에게 주어진다 (\\(a_{i5} = 1\\)). 위의 데이터 모델을 추가하여 학습데이터를 생성하고 선택 모형을 추정해보자. set.seed(22) N &lt;- 2000 menu &lt;- c(&quot;짬뽕&quot; = 1, &quot;짜장면&quot; = 2, &quot;삼선짬뽕&quot; = 3, &quot;삼선짜장&quot; = 4, &quot;안 먹어&quot; = 5) menu_name &lt;- names(menu) alpha_true &lt;- c(0, 4, 2, 6, -10) beta_true &lt;- -0.001 gamma_true &lt;- c(0, -0.1, 0, -0.1, 0) choice_df_2 &lt;- generate_price(N) %&gt;% complete(i, alternative = c(1L:5L), fill = list(price = 0)) %&gt;% inner_join(generate_age(N), by = &quot;i&quot;) %&gt;% inner_join( generate_availability(N) %&gt;% complete(i, alternative = c(1L:5L), fill = list(avail = 1)), by = c(&quot;i&quot;, &quot;alternative&quot;) ) %&gt;% filter(avail == 1) %&gt;% mutate( v = alpha_true[alternative] + beta_true * price + gamma_true[alternative] * age, e = rgumbel(n()), u = v + e ) %&gt;% mutate(menu_item = menu_name[alternative]) %&gt;% select(-avail) %&gt;% group_by(i) %&gt;% mutate(choice = if_else(u == max(u), 1L, 0L)) %&gt;% ungroup() fit_mnl_2 &lt;- mlogit(choice ~ price | age, data = choice_df_2, alt.var = &quot;alternative&quot;, chid.var = &quot;i&quot;) estimate_df_2 &lt;- tibble( term = names(fit_mnl_2$coefficients), estimate = coef(fit_mnl_2), std.error = sqrt(diag(solve(-fit_mnl_2$hessian))), true = c(alpha_true[-1], beta_true, gamma_true[-1]) ) term estimate std.error true (Intercept):2 4.3214638 0.2259372 4e+00 (Intercept):3 2.0024255 0.3628928 2e+00 (Intercept):4 6.1329368 0.3252538 6e+00 (Intercept):5 -10.1673555 1.0700422 -1e+01 price -0.0010016 0.0000837 -1e-03 age:2 -0.1073444 0.0046892 -1e-01 age:3 0.0001294 0.0040858 0e+00 age:4 -0.1036771 0.0056020 -1e-01 age:5 0.0028766 0.0132920 0e+00 이제 추정된 모형을 토대로, 가격이 비싼 메뉴판이 주어졌을 때 손님의 선택 확률을 추정해보자. alpha_hat_2 &lt;- c(0, coef(fit_mnl_2)[c(&quot;(Intercept):2&quot;, &quot;(Intercept):3&quot;, &quot;(Intercept):4&quot;, &quot;(Intercept):5&quot;)]) beta_hat_2 &lt;- coef(fit_mnl_2)[c(&quot;price&quot;)] gamma_hat_2 &lt;- c(0, coef(fit_mnl_2)[c(&quot;age:2&quot;, &quot;age:3&quot;, &quot;age:4&quot;, &quot;age:5&quot;)]) new_choice_situation_3 &lt;- tribble( ~menu_item, ~price, &quot;짬뽕&quot;, 37000, &quot;짜장면&quot;, 36000, &quot;삼선짬뽕&quot;, 40000, &quot;삼선짜장&quot;, 39000, &quot;안 먹어&quot;, 0 ) %&gt;% mutate( alternative = menu[menu_item] ) menu_item price alternative 짬뽕 37000 1 짜장면 36000 2 삼선짬뽕 40000 3 삼선짜장 39000 4 안 먹어 0 5 pred_df6 &lt;- new_choice_situation_3 %&gt;% mutate(age = 45) %&gt;% mutate( v = alpha_hat_2[alternative] + beta_hat_2 * price + gamma_hat_2[alternative] * age, p = exp(v - max(v)) / sum(exp(v - max(v))) ) 위 결과, 손님이 어떠한 음식도 주문하지 않고 음식점을 나설 확률이 100%에 근접하게 추정된다. 2.4 Nested logit model Nested logit model은 multinomial logit model과는 달리, 어떠한 두 대안이 다른 대안들보다 더 강한 대체관계를 갖는 상황을 가정한다. 예를 들어, 메뉴를 짜장류와 짬뽕류로 구분하여 생각할 때, 삼선짜장이 주문 가능하지 않을 경우에 대체재로 짬뽕류 대신 같은 짜장류 내의 다른 메뉴를 선택하는 경향이 있다면, 이는 각 선택이 메뉴 종류(짜장류, 짬뽕류)로부터 얻어지는 효용성과 각각의 메뉴로부터 얻어지는 효용성에 모두 기반한다고 볼 수 있다. 대안 \\(1, \\ldots, J\\)가 \\(C\\)개의 그룹(nest)으로 구분된다고 가정하자. 이 때, \\(m_{jc}\\)를 대안 \\(j\\)가 그룹 \\(c\\)에 속하는 지에 대한 indicator variable이라 하자. \\[ m_{jc} = \\begin{cases} 1 &amp; \\text{if alternative } j \\text{ belongs to group } c,\\\\ 0 &amp; \\text{otherwise.} \\end{cases} \\] 이 때, \\(m_{jc}\\)는 아래와 같은 제약조건을 만족한다 하자. \\[ \\sum_{c = 1}^{C} m_{jc} = 1, \\; \\forall j \\] 즉, 각 대안은 하나의 그룹에만 속한다. NOTE: Nested logit model은 multilevel nested logit model로 확장 가능하나, 본 장에서는 two-level nested logit 모형만 다루기로 하자. \\(M_c\\)를 그룹 \\(c\\)에 속하는 대안들의 집합이라 하자. \\[ M_c = \\{j: m_{jc} = 1\\} \\] 2.4.1 Utility 2.4.2 Observable utility \\(v_{ij}\\)에 대한 모형은 위에서 살펴본 multinomial logit model의 경우와 차이가 없다 해도 무방하다. NOTE: 경우에 따라서, 각 그룹별로 observable utility의 parametric form을 다르게 구성하거나, 파라미터 값이 서로 다르다 가정하고 추정할 수 있다. 이를 구현함에 있어서, 모형을 추정하는 방법에 차이를 두기보다는 feature vector(\\(\\mathbf{x}_{ij}, \\mathbf{z}_i\\))를 확장하는 형태로 구현할 수 있다. 이에 대한 자세한 설명은 생략하기로 한다. 2.4.3 Unobservable utility Nested logit model이 multinomial logit model과 중요한 차이를 보이는 부분은 unobservable utility에 대한 가정이다. \\(j \\in M_c\\)인 대안 \\(j\\)에 대하여, \\(i\\)번째 선택상황에서의 unobservable utility \\(\\varepsilon_{ij}\\)는 아래와 같이 두 가지 부분으로 나뉜다. \\[ \\varepsilon_{ij} = \\varepsilon_{ij}^{(c)} + \\varepsilon_i^{(c)}, \\; j \\in M_c \\] 이 때, 서로 다른 그룹 \\(c, d\\)에 속한 두 대안 \\(j, k\\)간에는 unobservable utility가 독립이다. \\[ Cov(\\varepsilon_{ij}, \\varepsilon_{ik}) = 0, \\forall j \\in M_c, k \\in M_{d \\neq c} \\] 그에 반해, 같은 그룹에 속하는 두 대안 \\(j, k\\)간에는 unobservable utility간에 상관관계가 존재하는데, 이는 두 unobservable utility가 공통의 그룹 레벨의 unobserable utility \\(\\varepsilon_i^{(c)}\\)를 포함하기 때문이다. Unobservable utility 벡터 \\((\\varepsilon_{i1}, \\ldots, \\varepsilon_{iJ})\\)는 아래와 같은 cumulative distribution을 따른다고 가정한다. \\[ \\exp\\left(- \\sum_{c = 1}^{C} \\left(\\sum_{j \\in M_c} \\exp(-\\varepsilon_{ij} / \\lambda_c) \\right)^{\\lambda_c} \\right) = \\exp\\left(- \\sum_{c = 1}^{C} \\left(\\sum_{j \\in M_c} \\exp(-(\\varepsilon_{ij}^{(c)} + \\varepsilon_i^{(c)}) / \\lambda_c) \\right)^{\\lambda_c} \\right) \\] 이 때, \\(\\varepsilon_{ij}^{(c)}\\)는 아래와 같은 Gumbel distribution을 따른다. \\[ \\varepsilon_{ij}^{(c)} \\overset{i.i.d.}{\\sim} Gumbel(0, \\lambda_c) \\] 여기에서, \\(\\lambda_c \\in (0, 1]\\)는 nest \\(c\\)내 대안들간의 독립성을 나타내는 척도로써, 값이 0에 가까울수록 그룹 내 대안들간 대체재의 성격이 강하다. 모든 \\(c\\)에 대해 \\(\\lambda_c = 1\\)이면 IIA 가정이 성립한다고 볼 수 있다. NOTE: \\(\\lambda_c &gt; 1\\)인 경우, 같은 그룹 내의 대안들보다는 다른 그룹에 속한 대안들과 더 강한 대체재 관계가 있다고 볼 수 있다. 본 장에서는 해당 경우는 고려하지 않기로 하자. TO DO: \\(\\varepsilon_i^{(c)}\\) 분포에 대한 추가 설명 필요. 2.4.4 Choice probability 그룹 \\(c\\) 내의 대안이 선택됨을 안다고 가정할 때 (즉, 그룹 \\(c\\) 외의 대안이 선택될 확률이 0이라 가정할 때), 위 i.i.d. Gumbel distribution 가정에 따라, 대안 \\(j \\in M_c\\)가 선택될 확률은 아래와 같이 계산된다. \\[ E\\left[y_{ij} \\, \\left| \\, \\sum_{k \\in M_c} y_{ik} = 1, j \\in M_c\\right.\\right] = \\frac{\\exp(v_{ij} / \\lambda_c)}{\\sum_{j \\in M_c} \\exp(v_{ij} / \\lambda_c)} \\] 한편, 그룹 \\(c\\) 내의 대안이 선택되는 확률은 다음과 같다. \\[ E\\left[\\sum_{j \\in M_c } y_{ij} = 1\\right] = \\frac{\\left(\\sum_{j \\in M_c} \\exp(v_{ij} / \\lambda_c)\\right)^{\\lambda_c}}{\\sum_{d = 1}^{C} \\left(\\sum_{j \\in M_d} \\exp(v_{ij} / \\lambda_d)\\right)^{\\lambda_d}} \\] 이 때, 대안 \\(j \\in M_c\\)가 선택상황 \\(i\\)에서 선택될 확률은, 위 두 확률을 곱하여 계산한다. \\[ E\\left[y_{ij} \\, \\left| \\, j \\in M_c\\right.\\right] = \\frac{\\exp(v_{ij} / \\lambda_c)}{\\sum_{j \\in M_c} \\exp(v_{ij} / \\lambda_c)} \\frac{\\left(\\sum_{j \\in M_c} \\exp(v_{ij} / \\lambda_c)\\right)^{\\lambda_c}}{\\sum_{d = 1}^{C} \\left(\\sum_{j \\in M_d} \\exp(v_{ij} / \\lambda_d)\\right)^{\\lambda_d}}\\\\ = \\frac{\\exp(v_{ij} / \\lambda_c) \\left(\\sum_{j \\in M_c} \\exp(v_{ij} / \\lambda_c)\\right)^{\\lambda_c - 1}}{\\sum_{d = 1}^{C} \\left(\\sum_{j \\in M_d} \\exp(v_{ij} / \\lambda_d)\\right)^{\\lambda_d}} \\] 2.4.5 예제 앞 절에서 살펴보았던 첫 예제에서, 짬뽕과 삼선짬뽕을 “짬뽕류”(\\(c = 1\\)), 짜장면과 삼선짜장을 “짜장류”(\\(c = 2\\))로 분류하고, unobservable utility에 대한 데이터 모델을 변경하여 데이터를 생성하자. 2.4.5.1 데이터 모델 대안 \\(j = 1\\): 짬뽕 \\(j = 2\\): 짜장면 \\(j = 3\\): 삼선짬뽕 \\(j = 4\\): 삼선짜장 Unobservable utility \\(\\varepsilon_{ij}^{(1)} \\overset{i.i.d.}{\\sim} Gumbel(0, \\frac{1}{\\sqrt{2}})\\), for \\(j = 1, 3\\) \\(\\varepsilon_{ij}^{(2)} \\overset{i.i.d.}{\\sim} Gumbel(0, \\frac{1}{\\sqrt{2}})\\), for \\(j = 2, 4\\) \\(\\varepsilon_{i}^{(1)} \\overset{i.i.d.}{\\sim} Gumbel(0, \\frac{1}{\\sqrt{2}})\\) \\(\\varepsilon_{i}^{(2)} \\overset{i.i.d.}{\\sim} Gumbel(0, \\frac{1}{\\sqrt{2}})\\) TO DO: unobservale utility 데이터 모델 재검토 필요. set.seed(2) N &lt;- 2000 menu &lt;- c( &quot;짬뽕&quot; = 1, &quot;짜장면&quot; = 2, &quot;삼선짬뽕&quot; = 3, &quot;삼선짜장&quot; = 4 ) menu_nest &lt;- c( &quot;짬뽕&quot; = &quot;짬뽕류&quot;, &quot;짜장면&quot; = &quot;짜장류&quot;, &quot;삼선짬뽕&quot; = &quot;짬뽕류&quot;, &quot;삼선짜장&quot; = &quot;짜장류&quot; ) menu_name &lt;- names(menu) alpha_true &lt;- c(0, 4, 2, 6) beta_true &lt;- -0.001 gamma_true &lt;- c(0,-0.1, 0,-0.1) nested_choice_df &lt;- generate_price(N) %&gt;% inner_join(generate_age(N), by = &quot;i&quot;) %&gt;% inner_join(generate_availability(N), by = c(&quot;i&quot;, &quot;alternative&quot;)) %&gt;% filter(avail == 1) %&gt;% mutate(menu_item = menu_name[alternative], group = menu_nest[menu_item]) %&gt;% mutate(v = alpha_true[alternative] + beta_true * price + gamma_true[alternative] * age, e_ijc = rgumbel(n(), 0, 1 / sqrt(2))) %&gt;% group_by(i, group) %&gt;% mutate(e_ic = rgumbel(1L, 0, 1 / sqrt(2))) %&gt;% ungroup() %&gt;% mutate(u = v + e_ijc + e_ic) %&gt;% select(-c(avail, group)) %&gt;% group_by(i) %&gt;% mutate(choice = if_else(u == max(u), 1L, 0L)) %&gt;% ungroup() 2.4.5.2 모델 추정 Nested logit model 역시 R 패키지 {mlogit}의 함수 mlogit()을 이용하여 추정할 수 있다. 이 때, nests 파라미터값을 통해 alt.var 파라미터에 지정된 컬럼의 값을 묶는다. fit_nest &lt;- mlogit(choice ~ price | age, data = nested_choice_df, alt.var = &quot;alternative&quot;, chid.var = &quot;i&quot;, nests = list(&quot;짬뽕류&quot; = c(1, 3), &quot;짜장류&quot; = c(2, 4))) estimate_nest_df &lt;- tibble( term = names(fit_nest$coefficients), estimate = coef(fit_nest), std.error = sqrt(diag(solve(-fit_nest$hessian))), true = c(alpha_true[-1], beta_true, gamma_true[-1], 1 / sqrt(2), 1 / sqrt(2)) ) term estimate std.error true (Intercept):2 3.6424590 0.1985348 4.0000000 (Intercept):3 2.1632391 0.3796305 2.0000000 (Intercept):4 5.4217769 0.3273083 6.0000000 price -0.0008896 0.0001474 -0.0010000 age:2 -0.0911395 0.0038780 -0.1000000 age:3 -0.0031917 0.0021977 0.0000000 age:4 -0.0891393 0.0041364 -0.1000000 iv:짬뽕류 0.5121572 0.0974558 0.7071068 iv:짜장류 0.6074646 0.1172593 0.7071068 TO DO: TRUE 데이터 모델 재검토 필요. 여기에서 사용된 데이터는 IIA 가정을 위배하므로, multinomial logit model은 nested logit model보다 데이터에 대한 설명력이 부족할 것이다. Log-likelihood ratio test를 이용하여 이를 확인하여 보자. fit_mnl_wrong &lt;- fit_nest %&gt;% update(nests = NULL) lrtest(fit_nest, fit_mnl_wrong) ## Likelihood ratio test ## ## Model 1: choice ~ price | age ## Model 2: choice ~ price | age ## #Df LogLik Df Chisq Pr(&gt;Chisq) ## 1 9 -1578.5 ## 2 7 -1586.8 -2 16.513 0.0002595 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 위 결과, 해당 데이터에 대해 nested logit model이 multinomial logit model보다 유의하게 우수한 설명력을 지님을 확인할 수 있다. 2.4.5.3 확률 추정 추정된 모형에 기반하여, 다음과 같은 메뉴판이 주어졌을 때 손님의 선택을 예측해보자. alpha_hat &lt;- c(0, coef(fit_nest)[c(&quot;(Intercept):2&quot;, &quot;(Intercept):3&quot;, &quot;(Intercept):4&quot;)]) beta_hat &lt;- coef(fit_nest)[c(&quot;price&quot;)] gamma_hat &lt;- c(0, coef(fit_nest)[c(&quot;age:2&quot;, &quot;age:3&quot;, &quot;age:4&quot;)]) iv_hat &lt;- coef(fit_nest)[c(&quot;iv:짬뽕류&quot;, &quot;iv:짜장류&quot;)] %&gt;% set_names(c(&quot;짬뽕류&quot;, &quot;짜장류&quot;)) new_choice_situation &lt;- tribble( ~menu_item, ~price, &quot;짬뽕&quot;, 7000, &quot;짜장면&quot;, 6000, &quot;삼선짬뽕&quot;, 10000, &quot;삼선짜장&quot;, 9000 ) %&gt;% mutate( alternative = menu[menu_item], group = menu_nest[menu_item] ) menu_item price alternative group 짬뽕 7000 1 짬뽕류 짜장면 6000 2 짜장류 삼선짬뽕 10000 3 짬뽕류 삼선짜장 9000 4 짜장류 반면, 손님의 나이가 45세인 경우의 선택확률 예측은 아래와 같다. observable_utility &lt;- new_choice_situation %&gt;% mutate(age = 45) %&gt;% mutate( lambda = iv_hat[group], v = alpha_hat[alternative] + beta_hat * price + gamma_hat[alternative] * age ) prob_within_nest &lt;- observable_utility %&gt;% group_by(group) %&gt;% mutate( p_within_nest = exp((v - max(v)) / lambda) / sum(exp((v - max(v)) / lambda)) ) %&gt;% ungroup() prob_nest &lt;- observable_utility %&gt;% group_by(group) %&gt;% summarize(v_nest = sum(exp(v / lambda))) %&gt;% mutate( lambda = iv_hat[group], p_nest = v_nest ^ lambda / sum(v_nest ^ lambda) ) %&gt;% select(-lambda) pred_df7 &lt;- prob_within_nest %&gt;% inner_join(prob_nest, by = &quot;group&quot;) %&gt;% mutate( p = p_within_nest * p_nest ) 만약 삼선짜장이 메뉴에서 제외된다면 않다면, 선택확률은 아래와 같이 달라진다. observable_utility1 &lt;- observable_utility %&gt;% filter(menu_item != &quot;삼선짜장&quot;) prob_within_nest1 &lt;- observable_utility1 %&gt;% group_by(group) %&gt;% mutate(p_within_nest = exp((v - max(v)) / lambda) / sum(exp((v - max(v)) / lambda))) %&gt;% ungroup() prob_nest1 &lt;- observable_utility1 %&gt;% group_by(group) %&gt;% summarize(v_nest = sum(exp(v / lambda))) %&gt;% mutate( lambda = iv_hat[group], p_nest = v_nest ^ lambda / sum(v_nest ^ lambda) ) %&gt;% select(-lambda) pred_df8 &lt;- prob_within_nest1 %&gt;% inner_join(prob_nest1, by = &quot;group&quot;) %&gt;% mutate( p = p_within_nest * p_nest ) 이 때, multinomial logit model의 경우와 달리, 삼선짜장을 원했던 손님들 대부분이 짜장면을 선택할 것으로 예측됨을 볼 수 있다. "],["censored-data.html", "Chapter 3 Censored data 3.1 Right censored data 3.2 Left censored data 3.3 Interval data", " Chapter 3 Censored data library(ggplot2) library(ggtext) library(ggrepel) library(gt) library(dplyr) library(tidyr) library(stringr) library(rlang) library(purrr) 확률분포나 통계모형을 추정할 때 가장 흔히 사용되는 방법이 최우추정법(maximum likelihood estimation)이다. 이 때 likelihood는 주어진 데이터가 어떠한 확률분포로부터 관측될 결합확률(joint probability)을 뜻한다. 이 때, likelihood 함수는 주어진 데이터의 성격에 맞게 정의되어야 한다. 기초 통계 수업에서는 주로 정확한(precise) 값이 관측된다고 가정한다. 하지만, 경우에 따라 분포를 알고자 하는 값이 정확하게 관측되지 못하고, 단지 정확한 실제값이 어떤 범위 내에 존재하는지만 알 수 있는 경우가 있다. 예를 들어, 매일 아침 10개의 재고가 존재하는 제품에 대해 기록된 일간 판매량을 토대로 일간 수요의 분포를 추정한다고 하자. 이 때, 제품 재고 10개가 모두 소진된 날의 경우, 판매량은 10개이지만 실제 수요는 “10개보다 크거나 같았다”와 같이 정확한 수요값이 존재했을 범위를 고려하는 것이 합리적인데, 이는 재고가 더 많이 존재했을 경우 더 많은 판매가 이루어졌을 가능성이 있기 때문이다. 이러한 경우, 관측된 범위 데이터를 censored data라 한다. 본 장에서는 이러한 censored data가 존재할 때 최우추정법을 위한 likelihood 함수를 어떻게 정의하는지 살펴보자. 3.1 Right censored data Right censored data는 실제값이 존재하는 범위의 lower bound가 관측되는 경우이다. 바로 앞에서 언급한 예와 같이, 재고량이 10개인 제품이 모두 판매되어 품절되었을 때, 제품에 대한 수요에 대한 lower bound 10은 관측되지만, 실제 수요가 11개였을지 20개였을 지는 알 수 없는 경우이다. 아래와 같이 데이터를 정의해보자. \\(N\\): 전체 관측 데이터 수. \\(y_i\\): \\(i\\)번째 관측치에 대응하는 실제값. 이 변수는 항상 관측 가능한 변수가 아니다. 예를 들어, \\(i\\)번째 날짜에 해당 일자에 대한 재고 10개가 모두 판매되어 품절되었을 경우, 재고가 충분했다면 15개가 판매되었을 것이라면 \\(y_i = 15\\)이지만, 이 실제값은 관측 가능한 값이 아니다. \\(\\tilde{y}_i\\): \\(i\\)번째 관측치에 대응하는 관측값. 위에서 언급한 경우와 같이 \\(i\\)번째 날짜에 해당 일자에 대한 재고 10개가 모두 판매되어 품절되었을 경우, \\(\\tilde{y}_i = 10\\)으로 관측된다. \\(r_i\\): \\(i\\)번째 관측치가 right censored data인지 아닌지 여부에 대한 지시변수. \\(i\\)번째 관측치가 right censored data라면 \\(r_i = 1\\), 아니라면 \\(r_i = 0\\)으로 관측된다. 이 지시변수는 항상 관측 가능한 값이다. 예를 들어, \\(i\\)번째 날짜에 재고가 소진되어 더 이상 제품 판매가 불가능했다면, \\(r_i = 1\\)로 관측된다. 이 때, \\(y_i\\)와 \\(\\tilde{y}_i\\)는 \\(r_i\\)값에 따라 아래와 같은 관계를 보인다. \\[ \\begin{cases} y_i = \\tilde{y}_i &amp; \\text{if } r_i = 0 \\\\ y_i \\geq \\tilde{y}_i &amp; \\text{if } r_i = 1 \\end{cases} \\] 전체 관측 데이터 \\(D\\)는 아래와 같이 \\(N\\)개의 \\(\\tilde{y}_i\\)와 \\(r_i\\)의 쌍으로 이루어진다. \\[ D = \\left\\{(\\tilde{y}_i, r_i): i = 1, \\ldots, N\\right\\} \\] 해당 관측 데이터에 대한 likelihood값의 계산은 실제값 변수 \\(Y\\)가 이산형(discrete) 변수인지 연속형(continuous) 변수인지에 따라 달라진다. 3.1.1 이산형 변수 변수 \\(Y\\)에 대한 확률질량함수 \\(P(Y = y)\\)를 이용하여 관측 데이터의 likelihood를 아래와 같이 정의된다. \\[ L(D) = \\prod_{i = 1}^{N} P(Y = \\tilde{y}_i)^{1 - r_i} \\left(1 - P(Y &lt; \\tilde{y}_i)\\right)^{r_i}\\\\ = \\prod_{i = 1}^{N} P(Y = \\tilde{y}_i)^{1 - r_i} P(Y \\geq \\tilde{y}_i)^{r_i} \\] 즉, 정확한 실제값을 관측한 경우 (\\(r_i = 0\\)), 확률질량함수 \\(P(Y = \\tilde{y}_i)\\)값이 해당 값을 관측할 likelihood이다. 반면, right censored data인 경우 (\\(r_i = 1\\)), 관측된 정보는 “\\(y_i\\)가 \\(\\tilde{y}_i\\)보다 작지 않다”이므로, 변수 \\(Y\\)가 \\(\\tilde{y}_i\\)보다 작지 않을 확률 \\(1 - P(y_i &lt; \\tilde{y}_i)\\)를 likelihood로 사용한다. 3.1.1.1 예제 유통기한이 단 하루인 제품이 있다고 하자. 100일동안 매일 아침 가게 문을 열 때 제품을 10개를 생산해서 준비해두고, 매일 저녁 가게 문을 닫을 때까지 제품이 몇 개 팔렸는지를 기록해두었다 하자. 각 날짜의 제품의 수요는 서로 독립이며 평균(\\(\\lambda\\))이 7인 Poisson distribution을 따른다고 가정하자. 단, 판매자는 실제 평균 수요를 알지 못하고, 100일동안 관측된 데이터를 이용하여 수요의 분포를 추정하려고 한다. \\[ y_i \\overset{i.i.d.}{\\sim} Pois(7) \\] set.seed(30) N &lt;- 100 lambda_true &lt;- 7 daily_inventory &lt;- 10L observed &lt;- tibble( day = seq_len(N), daily_sales = pmin(rpois(N, 7), daily_inventory), sold_out = if_else(daily_sales &lt; daily_inventory, 0, 1) ) Table 3.1: 첫 열흘 동안의 판매량 기록 day daily_sales sold_out 1 4 0 2 7 0 3 6 0 4 6 0 5 6 0 6 4 0 7 10 1 8 5 0 9 10 1 10 4 0 위 데이터에서 daily_sales열이 \\(\\tilde{y}_i\\), sold_out열이 \\(r_i\\) 값을 저장한다. 실제 수요값이 10인 경우, 관측된 판매량과 실제 수요가 일치하지만, 판매자의 입장에서는 실제 수요가 더 이상 없었을 것이라는 사실을 알 수가 없다. 실제 수요가 더 있었다 하더라도 수요자의 입장에서 구매할 수가 없었으므로, 판매량은 동일하게 10이며, 따라서 판매자의 입장에서는 판매량이 10인 경우 right censored data라고 가정한다. 위 데이터를 이용하여 Poisson distribution의 파라미터 \\(\\lambda\\)의 maximum likelihood estimate을 구해보자. 우선, likelihood function을 아래와 같이 정의한다. \\[ L(D \\, | \\, \\lambda) = \\prod_{i = 1}^{100} \\left(\\frac{\\lambda ^ {\\tilde{y}_i} \\exp(-\\lambda)}{\\tilde{y}_i!}\\right)^{1 - r_i} \\left(1 - \\sum_{k = 0}^{\\tilde{y}_i - 1} \\frac{\\lambda ^ {k} \\exp(-\\lambda)}{k!} \\right) ^ {r_i} \\] 이 때, 주어진 데이터 모형으로부터 \\(r_i = 1\\) 일 때 \\(\\tilde{y}_i\\)값은 항상 10임을 추론할 수 있으므로, likelihood function은 아래와 같이 정리할 수 있다. \\[ L(D \\, | \\, \\lambda) = \\prod_{i = 1}^{100} \\left(\\frac{\\lambda ^ {\\tilde{y}_i} \\exp(-\\lambda)}{\\tilde{y}_i!}\\right)^{1 - r_i} \\, p^{r_i}\\\\ p = 1 - \\sum_{k = 0}^{9} \\frac{\\lambda ^ {k} \\exp(-\\lambda)}{k!} \\] Log-likelihood function은 아래와 같이 정리된다. \\[ logL(D \\, | \\, \\lambda) = \\sum_{i = 1}^{100} (1 - r_i) \\left(\\tilde{y}_i \\log \\lambda - \\lambda - \\sum_{k = 1} ^ {\\tilde{y}_i} \\log k\\right) + r_i \\log p\\\\ p = 1 - \\sum_{k = 0}^{9} \\frac{\\lambda ^ {k} \\exp(-\\lambda)}{k!} \\] fn_loglik &lt;- function(lambda, y, rc) { sum(dpois(y, lambda, log = TRUE) * (1 - rc) + ppois(y, lambda, lower.tail = FALSE, log.p = TRUE) * rc) } 위 log-likelihood function을 이용하여 수요분포 파라미터 \\(\\lambda\\)의 maximum likelihood estimate을 구해보자. y &lt;- observed %&gt;% pull(daily_sales) rc &lt;- observed %&gt;% pull(sold_out) res &lt;- optim( par = 5, fn = fn_loglik, control = list(fnscale = -1), hessian = TRUE, y = y, rc = rc ) ## Warning in optim(par = 5, fn = fn_loglik, control = list(fnscale = -1), : one-dimensional optimization by Nelder-Mead is unreliable: ## use &quot;Brent&quot; or optimize() directly lambda_estimate &lt;- res$par lambda_se &lt;- drop(sqrt(- 1 / res$hessian)) 추정된 파리미터값은 \\(\\hat{\\lambda} = 6.5410156\\)이며 표준오차는 \\(se(\\hat{\\lambda}) = 0.2599846\\)이다. 추정된 95% 신뢰구간은 위 그래프에서 실제 분포를 포함하는 것을 볼 수 있다. 3.1.2 연속형 변수 실제값 \\(y_i\\)가 분포함수 \\(F(y)\\)로부터 얻어진다고 하자. \\[ y_i \\overset{i.i.d.}{\\sim} F(y) \\] 또한 분포함수 \\(F(y)\\)에 대응하는 확률밀도함수를 \\(f(y)\\)라 하자. \\[ f(y) = \\frac{\\partial F(y)}{\\partial y} \\] 이 때, likelihood는 아래와 같이 정의된다. \\[ L(D) = \\prod_{i = 1}^{N} f(\\tilde{y}_i)^{1 - r_i} \\left(1 - F(\\tilde{y}_i)\\right)^{r_i} \\] 즉, 정확한 실제값을 관측한 경우 (\\(r_i = 0\\)), 확률밀도함수 \\(f(y)\\)값이 해당 값을 관측할 likelihood이다. 반면, right censored data인 경우 (\\(r_i = 1\\)), 확신할 수 있는 정보는 \\(y_i \\geq \\tilde{y}_i\\)이므로, \\[ P(y_i \\geq \\tilde{y}_i) = 1 - P(y_i &lt; \\tilde{y}_i)\\\\ = 1 - F(\\tilde{y}_i) \\] 가 해당 관측값을 얻을 likelihood이다. 연속형 변수에 대한 예제는 다음 절에서 left censored data를 다룰 때 함께 살펴보기로 하자. 3.2 Left censored data Left censored data는 실제값이 존재하는 범위의 upper bound가 관측되는 경우이다. 즉, 실제값이 어떠한 값보다 작거나 같았다라는 사실을 관측할 수 있으나, 정확한 실제값을 측정할 수는 없는 경우이다. 본 절에서는 연속형 변수에 대해서만 설명하기로 한다. Right censored data와 left censored data가 모두 존재하는 다음과 같은 예를 생각해보자. 구매자와 판매자 간에 각 거래에서 제품 가격은 판매자가 결정하여 제시한다고 가정하자. 구매자 각자는 본인이 희망하는 제품 가격이 상한이 있으며, 이를 희망가격이라 하자. 하지만, 이 희망가격은 판매자가 알 수 없으며, 따라서 판매자는 각 구매자의 희망 가격과는 독립적으로 제품 가격을 설정하여 제시한다. 이 때, 거래가 성사되기 위해서는 구매자가 희망했던 가격이 판매자가 제시한 제품 가격보다 높아야 한다. 즉, 구매자가 희망했던 가격보다 제품가격이 저렴할 때 거래가 성립한다. 만약 구매자가 희망했던 가격이 제품 가격보다 낮은 경우, 즉 제품 가격이 구매자의 희망보다 비싼 경우에는 거래가 성립하지 않는다. 위 예에서, 판매자가 여러 구매자들과의 거래를 통해, 구매자들의 희망가격의 분포를 추정하려 한다고 하자. 판매자의 입장에서 관측할 수 있는 데이터는 판매자 본인이 제시했던 제품 가격과 거래가 성사되었는지의 여부이다. 이 때, 거래 성사 여부에 따라 관측된 데이터가 right censored data인지 left censored data인지를 알 수 있다. 거래가 성사된 경우, 구매자의 희망가격(\\(y_i\\))의 정확한 값은 모르지만, 판매자의 제시가격(\\(\\tilde{y}_i\\))보다 높았다는 사실을 알게 되므로, right censored data이다. 거래가 성사되지 않은 경우, 구매자의 희망가격(\\(y_i\\))의 정확한 값은 모르지만, 판매자의 제시가격(\\(\\tilde{y}_i\\))보다 낮았다는 사실을 알게 되므로, left censored data이다. 앞 절에서 정의했던 수식 기호에 더하여, left censored data 지시변수를 아래와 같이 추가로 정의하자. \\(l_i\\): \\(i\\)번째 관측치가 left censored data인지 아닌지 여부에 대한 지시변수. \\(i\\)번째 관측치가 left censored data라면 \\(l_i = 1\\), 아니라면 \\(l_i = 0\\)으로 관측된다. 이 때, 관심변수의 실제값 \\(y_i\\)와 관측값 \\(\\tilde{y}_i\\)간의 관계는 지시변수 \\(r_i\\)와 \\(l_i\\)의 값에 따라 아래와 같다. \\[ \\begin{cases} y_i = \\tilde{y}_i &amp; \\text{if } \\, l_i = 0 \\, \\text{ and } \\, r_i = 0\\\\ y_i \\geq \\tilde{y}_i &amp; \\text{if } \\, l_i = 0 \\, \\text{ and } \\, r_i = 1\\\\ y_i \\leq \\tilde{y}_i &amp; \\text{if } \\, l_i = 1 \\, \\text{ and } \\, r_i = 0 \\end{cases} \\] 전체 관측 데이터 \\(D\\)는 아래와 같이 \\(N\\)개의 \\(\\tilde{y}_i\\)와 \\(l_i\\), 그리고 \\(r_i\\)의 쌍으로 이루어진다. \\[ D = \\left\\{(\\tilde{y}_i, l_i, r_i): i = 1, \\ldots, N\\right\\} \\] 이 때, likelihood는 아래와 같이 정의된다. \\[ L(D) = \\prod_{i = 1}^{N} f(\\tilde{y}_i)^{1 - l_i - r_i} F(\\tilde{y}_i)^{l_i} \\left(1 - F(\\tilde{y}_i)\\right)^{r_i} \\] 3.2.0.1 예제 앞에서 언급한 구매자의 희망가격 분포를 추정하는 예에 대해 실제 추정과정을 살펴보자. 구매자 각자는 서로 다른 희망가격을 마음에 품고 있는데, 이는 서로 독립이며 평균(\\(1 / \\lambda\\))이 10만원인 Exponential distribution을 따른다고 가정하자. 단, 판매자는 구매자의 평균 희망가격을 알지 못하고, 100명의 구매자와의 거래 시도를 통해 관측된 데이터를 이용하여 희망가격의 분포를 추정하려고 한다. \\[ y_i \\overset{i.i.d.}{\\sim} Exp(\\lambda) \\] 판매자가 각 구매자에게 무작위로 서로 다른 가격을 제시하였다고 하자. 이 때, 각 구매자에게 제시된 가격은 서로 독립이며 평균이 9이고 표준편차가 2인 정규분포로부터 추출된 무작위 샘플이라 하자. \\[ \\tilde{y}_i \\overset{i.i.d.}{\\sim} N(9, 2^2) \\] set.seed(31) N &lt;- 100 lambda_true &lt;- 1 / 10 mu_tilde &lt;- 9 sigma_tilde &lt;- 2 observed &lt;- tibble( customer = seq_len(N), upper_limit = rexp(N, lambda_true), offer = rnorm(N, mu_tilde, sigma_tilde), is_accepted = if_else(offer &lt;= upper_limit, 1, 0), is_rejected = if_else(offer &gt; upper_limit, 1, 0) ) Table 3.2: 첫 열 명의 구매자와의 거래 결과 customer upper_limit offer is_accepted is_rejected 1 0.4431583 10.908619 0 1 2 2.9419262 4.507467 0 1 3 3.1340344 9.060761 0 1 4 17.6789979 9.804956 1 0 5 1.6851793 12.316684 0 1 6 6.6046450 5.295574 1 0 7 3.0373506 12.301354 0 1 8 42.3869027 7.171987 1 0 9 26.9941280 10.744140 1 0 10 7.6777810 10.346243 0 1 위 데이터에서 is_accept는 거래가 성사되었으면 1, 아니면 0을 지니며, right censored data 지시변수 \\(r_i\\)에 대응한다. 또한 is_rejected는 거래가 성사되었으면 0, 아니면 1을 지니며, left censored data 지시변수 \\(l_i\\)에 대응한다. 각 관측치는 left censored data이거나 right censored data이며, 판매자가 구매자의 희망가격을 정확히 아는 경우는 없다고 보는 것이 합리적일 것이다. 따라서, likelihood function은 아래와 같이 정의된다. \\[ L(D \\, | \\, \\lambda) = \\prod_{i = 1}^{100} \\left(1 - \\exp(-\\lambda \\tilde{y}_i)\\right)^{l_i} \\left(\\exp(-\\lambda \\tilde{y}_i)\\right)^{r_i} \\] 그리고, log-likelihood function은 아래와 같이 정의된다. \\[ l(D \\, | \\, \\lambda) = \\sum_{i = 1}^{100} l_i \\log \\left(1 - \\exp(-\\lambda \\tilde{y}_i)\\right) - r_i \\lambda \\tilde{y}_i \\] fn_loglik &lt;- function(lambda, y, lc, rc) { sum(pexp(y, lambda, lower.tail = TRUE, log.p = TRUE) * lc + pexp(y, lambda, lower.tail = FALSE, log.p = TRUE) * rc) } 위 log-likelihood function을 이용하여 수요분포 파라미터 \\(\\lambda\\)의 maximum likelihood estimate을 구해보자. y &lt;- observed %&gt;% pull(offer) lc &lt;- observed %&gt;% pull(is_rejected) rc &lt;- observed %&gt;% pull(is_accepted) res &lt;- optim( par = 1 / 20, fn = fn_loglik, control = list(fnscale = -1), hessian = TRUE, y = y, lc = lc, rc = rc ) ## Warning in optim(par = 1/20, fn = fn_loglik, control = list(fnscale = -1), : one-dimensional optimization by Nelder-Mead is unreliable: ## use &quot;Brent&quot; or optimize() directly lambda_estimate &lt;- res$par lambda_se &lt;- drop(sqrt(- 1 / res$hessian)) 추정된 파리미터값은 \\(\\hat{\\lambda} = 0.0824023\\)이며 (즉, 추정된 평균 희망가격은 12.1355772만원이며) 표준오차는 \\(se(\\hat{\\lambda}) = 0.0117518\\)이다. 추정된 95% 신뢰구간은 위 그래프에서 실제 분포를 포함하는 것을 볼 수 있다. 3.3 Interval data Interval data는 실제값이 존재하는 범위의 lower bound와 upper bound가 모두 관측되는 경우이다. "]]
